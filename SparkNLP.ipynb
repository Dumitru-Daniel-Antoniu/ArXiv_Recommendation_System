{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd671ea",
   "metadata": {},
   "source": [
    "\n",
    "# arXiv Classification with Spark NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33ad6504-8a02-46cd-b3c0-fc8a7b61bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping pyspark as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping spark-nlp as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping pyspark as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping spark-nlp as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFiles removed: 480 (5818.4 MB)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.5.0\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark==3.5.0)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425400 sha256=9cdc1fa00ac7358fcb4376aa9b269593d0834d1ab36bd9632b522da1808ac9c1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dxljgs9e/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[2K  Attempting uninstall: py4j\n",
      "\u001b[2K    Found existing installation: py4j 0.10.9.7\n",
      "\u001b[2K    Uninstalling py4j-0.10.9.7:\n",
      "\u001b[2K      Successfully uninstalled py4j-0.10.9.7\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]m1/2\u001b[0m [pyspark]\n",
      "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.7 pyspark-3.5.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spark-nlp==5.5.0\n",
      "  Downloading spark_nlp-5.5.0-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Downloading spark_nlp-5.5.0-py2.py3-none-any.whl (620 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.8/620.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pyspark spark-nlp -y\n",
    "!pip uninstall pyspark spark-nlp -y  # Da, de 2 ori pentru siguranță\n",
    "\n",
    "!pip cache purge\n",
    "\n",
    "!pip install --no-cache-dir --force-reinstall pyspark==3.5.0\n",
    "!pip install --no-cache-dir --force-reinstall spark-nlp==5.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861a3363-3252-4b6e-a2a2-40f3a2f5f287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]\n",
      "PySpark version: 3.5.0\n",
      "Spark NLP version: 5.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/09 13:21:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark runtime version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import sparknlp\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")  \n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(f\"Spark runtime version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b05c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e542bb4e-ab11-41cd-bce3-e62dc5089ac4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.1.4 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      ":: resolution report :: resolve 1415ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.1.4 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   75  |   0   |   0   |   3   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e542bb4e-ab11-41cd-bce3-e62dc5089ac4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/14ms)\n",
      "26/01/09 12:41:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP initialized\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start(\n",
    "    memory=\"6g\"\n",
    ")\n",
    "\n",
    "print(\"Spark NLP initialized\")\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32cbd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DistilBertEmbeddings, ClassifierDLApproach\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca025da2-439f-4dc4-91d8-f9d78e953b7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.10/site-packages (25.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.10/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /home/ubuntu/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/.local/lib/python3.10/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /home/ubuntu/.local/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch) (59.6.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Invalid requirement: '[torch]': Expected package name at the start of dependency specifier\n",
      "    [torch]\n",
      "    ^\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.3.2\n",
      "  Using cached pyspark-3.3.2-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.5 (from pyspark==3.3.2)\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[2K  Attempting uninstall: py4j\n",
      "\u001b[2K    Found existing installation: py4j 0.10.9.7\n",
      "\u001b[2K    Uninstalling py4j-0.10.9.7:\n",
      "\u001b[2K      Successfully uninstalled py4j-0.10.9.7\n",
      "\u001b[2K  Attempting uninstall: pyspark\n",
      "\u001b[2K    Found existing installation: pyspark 3.5.0\n",
      "\u001b[2K    Uninstalling pyspark-3.5.0:\n",
      "\u001b[2K      Successfully uninstalled pyspark-3.5.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]m1/2\u001b[0m [pyspark]\n",
      "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sparknlp in /home/ubuntu/.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: spark-nlp in /home/ubuntu/.local/lib/python3.10/site-packages (from sparknlp) (5.1.4)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from sparknlp) (2.2.6)\n",
      "Found existing installation: pyspark 3.3.2\n",
      "Uninstalling pyspark-3.3.2:\n",
      "  Successfully uninstalled pyspark-3.3.2\n",
      "Found existing installation: spark-nlp 5.1.4\n",
      "Uninstalling spark-nlp-5.1.4:\n",
      "  Successfully uninstalled spark-nlp-5.1.4\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.5.0\n",
      "  Using cached pyspark-3.5.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.5.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[2K  Attempting uninstall: py4j\n",
      "\u001b[2K    Found existing installation: py4j 0.10.9.5\n",
      "\u001b[2K    Uninstalling py4j-0.10.9.5:\n",
      "\u001b[2K      Successfully uninstalled py4j-0.10.9.5\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pyspark]m1/2\u001b[0m [pyspark]\n",
      "\u001b[1A\u001b[2KSuccessfully installed py4j-0.10.9.7 pyspark-3.5.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spark-nlp==5.1.4\n",
      "  Using cached spark_nlp-5.1.4-py2.py3-none-any.whl.metadata (54 kB)\n",
      "Using cached spark_nlp-5.1.4-py2.py3-none-any.whl (540 kB)\n",
      "Installing collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.1.4\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ubuntu/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets) (3.3)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ubuntu/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas matplotlib seaborn numpy scikit-learn\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers\n",
    "!pip install transformers [torch]\n",
    "!pip install pyspark==3.3.2\n",
    "!pip install sparknlp\n",
    "!pip uninstall -y pyspark spark-nlp\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install spark-nlp==5.1.4\n",
    "!pip install datasets\n",
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:52:12.513859Z",
     "start_time": "2026-01-08T11:52:12.508662Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sparknlp\n",
    "import warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ad1a98-ef16-4f96-9ffa-46a98bb4a759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP initialized\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark NLP initialized\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b49f127f-f03a-4f12-b5ba-baab0f67c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, StopWordsRemover, StringIndexer, Tokenizer\n",
    "from pyspark.sql.functions import col, split, explode, count\n",
    "\n",
    "from sparknlp.annotator import ClassifierDLApproach, DistilBertEmbeddings, Tokenizer\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    Tokenizer,\n",
    "    StopWordsRemover,\n",
    "    HashingTF,\n",
    "    IDF\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48975aa7-659f-42b3-9c81-5ddb6b7f95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|categories     |\n",
      "+---------------+\n",
      "|hep-ph         |\n",
      "|math.CO cs.CG  |\n",
      "|physics.gen-ph |\n",
      "|math.CO        |\n",
      "|math.CA math.FA|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories count: 21564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|categories       |count|\n",
      "+-----------------+-----+\n",
      "|astro-ph         |16405|\n",
      "|hep-ph           |15547|\n",
      "|quant-ph         |13733|\n",
      "|astro-ph.CO      |10718|\n",
      "|hep-th           |10424|\n",
      "|astro-ph.SR      |8477 |\n",
      "|cond-mat.mes-hall|7550 |\n",
      "|cond-mat.mtrl-sci|7041 |\n",
      "|gr-qc            |5777 |\n",
      "|cond-mat.str-el  |5552 |\n",
      "|cs.IT math.IT    |5204 |\n",
      "|math.PR          |5149 |\n",
      "|math.AP          |4916 |\n",
      "|astro-ph.HE      |4872 |\n",
      "|math.CO          |4792 |\n",
      "|hep-ex           |4778 |\n",
      "|math.AG          |4211 |\n",
      "|nucl-th          |4110 |\n",
      "|cond-mat.supr-con|4026 |\n",
      "|astro-ph.GA      |3981 |\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories after split: 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==================================>                       (3 + 2) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|category          |count|\n",
      "+------------------+-----+\n",
      "|hep-ph            |35594|\n",
      "|hep-th            |32373|\n",
      "|quant-ph          |27568|\n",
      "|astro-ph.CO       |23096|\n",
      "|gr-qc             |21505|\n",
      "|math.MP           |19977|\n",
      "|math-ph           |19977|\n",
      "|astro-ph          |19875|\n",
      "|cond-mat.mes-hall |18280|\n",
      "|cond-mat.mtrl-sci |17331|\n",
      "|cond-mat.str-el   |15744|\n",
      "|cond-mat.stat-mech|15365|\n",
      "|astro-ph.SR       |14218|\n",
      "|math.CO           |11750|\n",
      "|nucl-th           |11741|\n",
      "|hep-ex            |11562|\n",
      "|math.PR           |11549|\n",
      "|astro-ph.HE       |11338|\n",
      "|math.AG           |11183|\n",
      "|cond-mat.supr-con |10420|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "df.select(\"categories\").show(5, truncate=False)\n",
    "\n",
    "unique_count = df.select(\"categories\").distinct().count()\n",
    "print(f\"Unique categories count: {unique_count}\")\n",
    "\n",
    "top_categories = df.groupBy(\"categories\") \\\n",
    "                   .agg(count(\"*\").alias(\"count\")) \\\n",
    "                   .orderBy(col(\"count\").desc()) \\\n",
    "                   .limit(20)\n",
    "\n",
    "top_categories.show(truncate=False)\n",
    "\n",
    "df_exploded = df.withColumn(\"category\", explode(split(col(\"categories\"), \" \")))\n",
    "\n",
    "unique_exploded = df_exploded.select(\"category\").distinct().count()\n",
    "print(f\"Unique categories after split: {unique_exploded}\")\n",
    "\n",
    "df_exploded.groupBy(\"category\") \\\n",
    "           .agg(count(\"*\").alias(\"count\")) \\\n",
    "           .orderBy(col(\"count\").desc()) \\\n",
    "           .show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684290b7-a5ec-42eb-9754-00ea52adb88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |label         |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "|  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n|hep-ph        |\n",
      "|  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n                                                                                                                                                                                            |math.CO       |\n",
      "|  The evolution of Earth-Moon system is described by the dark matter field\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\nwith this model very well and the general pattern of the evolution of the\\nMoon-Earth system described by this model agrees with geological and fossil\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\nbillion years ago, which is far beyond the Roche's limit. The result suggests\\nthat the tidal friction may not be the primary cause for the evolution of the\\nEarth-Moon system. The average dark matter field fluid constant derived from\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\nthat the Mars's rotation is also slowing with the angular acceleration rate\\nabout -4.38 x 10^(-22) rad s^(-2).\\n                                                                                                         |physics.gen-ph|\n",
      "|  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\\nsingle-source automata. The proof involves a bijection from these automata to\\ncertain marked lattice paths and a sign-reversing involution to evaluate the\\ndeterminant.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |math.CO       |\n",
      "|  In this paper we show how to compute the $\\Lambda_{\\alpha}$ norm, $\\alpha\\ge\\n0$, using the dyadic grid. This result is a consequence of the description of\\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |math.CA       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "          .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "          .select(\"abstract\", \"label\") \\\n",
    "          .na.drop()\n",
    "\n",
    "df_ml.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff2f421e-1a90-4c7c-976a-3ccb857a31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"label\",\n",
    "    outputCol=\"labelIndex\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    inputCol=\"abstract\",\n",
    "    outputCol=\"words\"\n",
    ")\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"words\",\n",
    "    outputCol=\"filtered_words\"\n",
    ")\n",
    "\n",
    "hashingTF = HashingTF(\n",
    "    inputCol=\"filtered_words\",\n",
    "    outputCol=\"rawFeatures\",\n",
    "    numFeatures=20000\n",
    ")\n",
    "\n",
    "idf = IDF(\n",
    "    inputCol=\"rawFeatures\",\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"labelIndex\",\n",
    "    maxIter=20\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    hashingTF,\n",
    "    idf,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c10020f-acc1-440f-83dc-8e6c4a65316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 12:42:39 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/01/09 12:51:24 WARN DAGScheduler: Broadcasting large task binary with size 22.5 MiB\n",
      "[Stage 47:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+----------+\n",
      "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |label          |prediction|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+----------+\n",
      "|  \"Einstein-aether\" theory is a generally covariant theory of gravity\\ncontaining a dynamical preferred frame. This article continues an examination\\nof effects on the motion of binary pulsar systems in this theory, by\\nincorporating effects due to strong fields in the vicinity of neutron star\\npulsars. These effects are included through an effective approach, by treating\\nthe compact bodies as point particles with nonstandard, velocity dependent\\ninteractions parametrized by dimensionless \"sensitivities\". Effective\\npost-Newtonian equations of motion for the bodies and the radiation damping\\nrate are determined. More work is needed to calculate values of the\\nsensitivities for a given fluid source, so precise constraints on the theory's\\ncoupling constants cannot yet be stated. It is shown, however, that strong\\nfield effects will be negligible given current observational uncertainties if\\nthe dimensionless couplings are less than roughly 0.01 and two conditions that\\nmatch the PPN parameters to those of pure general relativity are imposed. In\\nthis case, weak field results suffice and imply one further condition on the\\ncouplings. Thus, there exists a one-parameter family of Einstein-aether\\ntheories with \"small-enough\" couplings that passes all current observational\\ntests. No conclusion can yet be reached for large couplings.\\n|gr-qc          |1.0       |\n",
      "|  \"Granular elasticity,\" useful for calculating static stress distributions in\\ngranular media, is generalized by including the effects of slowly moving,\\ndeformed grains. The result is a hydrodynamic theory for granular solids that\\nagrees well with models from soil mechanics.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |cond-mat.soft  |24.0      |\n",
      "|  \"I have dealt with many different transformations with various periods of\\ntime, but the quickest that I have met was my own transformation in one moment\\nfrom a physicist to a chemist.\"\\n  Ernest Rutherford (Nobel Banquet, 1908)\\n  This article is about how Ernest Rutherford (1871-1937) got the 1908 Nobel\\nPrize in Chemistry and why he did not get a second Prize for his subsequent\\noutstanding discoveries in physics, specially the discovery of the atomic\\nnucleus and the proton. Who were those who nominated him and who did he\\nnominate for the Nobel Prizes.\\n  In order to put the Prize issue into its proper context, I will briefly\\ndescribe Rutherford's whereabouts.\\n  Rutherford, an exceptionally gifted scientist who revolutionized chemistry\\nand physics, was moulded in the finest classical tradition. What were his\\nopinions on some scientific issues such as Einstein's photon, uncertainty\\nrelations and the future prospects for atomic energy? What would he have said\\nabout the \"Theory of Everything\"?\\n                                                                                                                                                                                                                                                                                                                                            |physics.hist-ph|36.0      |\n",
      "|  \"Quantum trajectories\" are solutions of stochastic differential equations of\\nnon-usual type. Such equations are called \"Belavkin\" or \"Stochastic\\nSchr\\\"odinger Equations\" and describe random phenomena in continuous\\nmeasurement theory of Open Quantum System. Many recent investigations deal with\\nthe control theory in such model. In this article, stochastic models are\\nmathematically and physically justified as limit of concrete discrete\\nprocedures called \"Quantum Repeated Measurements\". In particular, this gives a\\nrigorous justification of the Poisson and diffusion approximation in quantum\\nmeasurement theory with control. Furthermore we investigate some examples using\\ncontrol in quantum mechanics.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |math.PR        |2.0       |\n",
      "|  $C^*$-algebraic Weyl quantization is extended by allowing also degenerate\\npre-symplectic forms for the Weyl relations with infinitely many degrees of\\nfreedom, and by starting out from enlarged classical Poisson algebras. A\\npowerful tool is found in the construction of Poisson algebras and\\nnon-commutative twisted Banach-$*$-algebras on the stage of measures on the not\\nlocally compact test function space. Already within this frame strict\\ndeformation quantization is obtained, but in terms of Banach-$*$-algebras\\ninstead of $C^*$-algebras. Fourier transformation and representation theory of\\nthe measure Banach-$*$-algebras are combined with the theory of continuous\\nprojective group representations to arrive at the genuine $C^*$-algebraic\\nstrict deformation quantization in the sense of Rieffel and Landsman. Weyl\\nquantization is recognized to depend in the first step functorially on the (in\\ngeneral) infinite dimensional, pre-symplectic test function space; but in the\\nsecond step one has to select a family of representations, indexed by the\\ndeformation parameter $\\hbar$. The latter ambiguity is in the present\\ninvestigation connected with the choice of a folium of states, a structure,\\nwhich does not necessarily require a Hilbert space representation.\\n                                                                         |math-ph        |6.0       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions.select(\"abstract\", \"label\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2cc371b-c4a8-403a-aa34-8868ec8f52c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 12:51:26 WARN DAGScheduler: Broadcasting large task binary with size 22.6 MiB\n",
      "[Stage 48:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a64762-4b85-48ae-ba11-b2c4ef6ae388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 12:51:37 WARN DAGScheduler: Broadcasting large task binary with size 22.5 MiB\n",
      "[Stage 50:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|labelIndex|prediction|count|\n",
      "+----------+----------+-----+\n",
      "|       0.0|       0.0| 4138|\n",
      "|       0.0|       1.0|  211|\n",
      "|       0.0|       2.0|   25|\n",
      "|       0.0|       3.0|   43|\n",
      "|       0.0|       4.0|   63|\n",
      "|       0.0|       5.0|    6|\n",
      "|       0.0|       6.0|   47|\n",
      "|       0.0|       7.0|    5|\n",
      "|       0.0|       8.0|   16|\n",
      "|       0.0|       9.0|    4|\n",
      "|       0.0|      10.0|   22|\n",
      "|       0.0|      11.0|   13|\n",
      "|       0.0|      12.0|    2|\n",
      "|       0.0|      13.0|   40|\n",
      "|       0.0|      14.0|    5|\n",
      "|       0.0|      16.0|    6|\n",
      "|       0.0|      17.0|    2|\n",
      "|       0.0|      18.0|  180|\n",
      "|       0.0|      19.0|    2|\n",
      "|       0.0|      20.0|    1|\n",
      "|       0.0|      21.0|    2|\n",
      "|       0.0|      22.0|    3|\n",
      "|       0.0|      23.0|  115|\n",
      "|       0.0|      24.0|    1|\n",
      "|       0.0|      25.0|    3|\n",
      "|       0.0|      27.0|    1|\n",
      "|       0.0|      28.0|    1|\n",
      "|       0.0|      29.0|    1|\n",
      "|       0.0|      31.0|   73|\n",
      "|       0.0|      32.0|    1|\n",
      "|       0.0|      33.0|    6|\n",
      "|       0.0|      34.0|   38|\n",
      "|       0.0|      36.0|   24|\n",
      "|       0.0|      37.0|    1|\n",
      "|       0.0|      38.0|    2|\n",
      "|       0.0|      39.0|    5|\n",
      "|       0.0|      40.0|    8|\n",
      "|       0.0|      42.0|    1|\n",
      "|       0.0|      44.0|    1|\n",
      "|       0.0|      46.0|    1|\n",
      "|       0.0|      48.0|    1|\n",
      "|       0.0|      49.0|    1|\n",
      "|       0.0|      50.0|    1|\n",
      "|       0.0|      51.0|    6|\n",
      "|       0.0|      52.0|    1|\n",
      "|       0.0|      54.0|    1|\n",
      "|       0.0|      55.0|    5|\n",
      "|       0.0|      57.0|    1|\n",
      "|       0.0|      58.0|    2|\n",
      "|       0.0|      62.0|    1|\n",
      "+----------+----------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 12:51:48 WARN DAGScheduler: Broadcasting large task binary with size 22.5 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "confusion = predictions.groupBy(\"labelIndex\", \"prediction\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"labelIndex\", \"prediction\")\n",
    "\n",
    "confusion.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78f1fcc6-2661-4635-91cc-2e6cc91dd4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 hep-ph\n",
      "1 hep-th\n",
      "2 quant-ph\n",
      "3 astro-ph\n",
      "4 astro-ph.CO\n",
      "5 cond-mat.mes-hall\n",
      "6 gr-qc\n",
      "7 cond-mat.mtrl-sci\n",
      "8 astro-ph.SR\n",
      "9 cond-mat.str-el\n",
      "10 math-ph\n",
      "11 cond-mat.stat-mech\n",
      "12 math.PR\n",
      "13 astro-ph.HE\n",
      "14 cond-mat.supr-con\n",
      "15 math.AG\n",
      "16 math.CO\n",
      "17 math.AP\n",
      "18 nucl-th\n",
      "19 cs.IT\n",
      "20 math.DG\n",
      "21 astro-ph.GA\n",
      "22 math.NT\n",
      "23 hep-ex\n",
      "24 cond-mat.soft\n",
      "25 physics.optics\n",
      "26 math.DS\n",
      "27 math.FA\n",
      "28 astro-ph.EP\n",
      "29 cond-mat.quant-gas\n",
      "30 math.RT\n",
      "31 hep-lat\n",
      "32 math.GT\n",
      "33 astro-ph.IM\n",
      "34 nucl-ex\n",
      "35 math.CA\n",
      "36 physics.gen-ph\n",
      "37 math.GR\n",
      "38 math.ST\n",
      "39 cond-mat.other\n",
      "40 physics.atom-ph\n",
      "41 math.OC\n",
      "42 math.NA\n",
      "43 physics.flu-dyn\n",
      "44 cs.AI\n",
      "45 cs.NI\n",
      "46 physics.soc-ph\n",
      "47 math.RA\n",
      "48 cs.DS\n",
      "49 cond-mat.dis-nn\n",
      "50 math.CV\n",
      "51 physics.ins-det\n",
      "52 math.OA\n",
      "53 math.QA\n",
      "54 math.AT\n",
      "55 physics.plasm-ph\n",
      "56 cs.LO\n",
      "57 math.AC\n",
      "58 stat.ME\n",
      "59 cs.CR\n",
      "60 nlin.CD\n",
      "61 cs.LG\n",
      "62 math.LO\n",
      "63 physics.chem-ph\n",
      "64 q-bio.PE\n",
      "65 physics.bio-ph\n",
      "66 cs.CV\n",
      "67 nlin.SI\n",
      "68 cs.DC\n",
      "69 math.MG\n",
      "70 stat.AP\n",
      "71 physics.class-ph\n",
      "72 cs.DM\n",
      "73 cs.CC\n",
      "74 math.SP\n",
      "75 math.SG\n",
      "76 cs.GT\n",
      "77 physics.acc-ph\n",
      "78 physics.data-an\n",
      "79 cs.SE\n",
      "80 cs.DB\n",
      "81 physics.comp-ph\n",
      "82 nlin.PS\n",
      "83 stat.ML\n",
      "84 cs.OH\n",
      "85 math.GM\n",
      "86 math.CT\n",
      "87 q-bio.QM\n",
      "88 physics.geo-ph\n",
      "89 cs.IR\n",
      "90 cs.CG\n",
      "91 math.GN\n",
      "92 math.KT\n",
      "93 cs.SI\n",
      "94 q-bio.NC\n",
      "95 physics.hist-ph\n",
      "96 cs.PL\n",
      "97 q-bio.BM\n",
      "98 cs.NE\n",
      "99 math.HO\n",
      "100 q-bio.MN\n",
      "101 nlin.AO\n",
      "102 cs.CL\n",
      "103 cs.FL\n",
      "104 stat.CO\n",
      "105 q-fin.GN\n",
      "106 q-fin.ST\n",
      "107 physics.ao-ph\n",
      "108 cs.SY\n",
      "109 q-fin.PR\n",
      "110 cs.CY\n",
      "111 cs.RO\n",
      "112 cs.DL\n",
      "113 cs.HC\n",
      "114 physics.med-ph\n",
      "115 physics.ed-ph\n",
      "116 cs.CE\n",
      "117 q-bio.GN\n",
      "118 physics.pop-ph\n",
      "119 physics.space-ph\n",
      "120 cs.SC\n",
      "121 physics.atm-clus\n",
      "122 cs.NA\n",
      "123 q-fin.RM\n",
      "124 cs.AR\n",
      "125 q-fin.PM\n",
      "126 q-fin.TR\n",
      "127 cs.MM\n",
      "128 q-fin.CP\n",
      "129 q-bio.CB\n",
      "130 cs.MA\n",
      "131 cs.PF\n",
      "132 q-bio.SC\n",
      "133 cs.MS\n",
      "134 nlin.CG\n",
      "135 q-bio.TO\n",
      "136 q-bio.OT\n",
      "137 cs.GR\n",
      "138 cs.OS\n",
      "139 cs.SD\n",
      "140 cs.ET\n",
      "141 stat.OT\n",
      "142 cs.GL\n",
      "143 eess.SY\n",
      "144 q-fin.EC\n"
     ]
    }
   ],
   "source": [
    "labels = model.stages[0].labels  # din StringIndexer\n",
    "for i, label in enumerate(labels):\n",
    "    print(i, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a1e600-4bc1-4fc3-8aa1-821da20d1bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 12:51:49 WARN DAGScheduler: Broadcasting large task binary with size 22.6 MiB\n",
      "[Stage 53:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 weighted: 0.559227255353717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "f1_weighted = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "print(\"F1 weighted:\", f1_weighted.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "753dcc2a-06ec-4c28-8268-36a550896bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 12:52:00 WARN DAGScheduler: Broadcasting large task binary with size 22.5 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "example = predictions.select(\n",
    "    \"abstract\",\n",
    "    \"label\",\n",
    "    \"prediction\"\n",
    ").limit(1).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd8ea4c3-d064-4796-8b1b-fe060ce9e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSTRACT:\n",
      "  \"Einstein-aether\" theory is a generally covariant theory of gravity\n",
      "containing a dynamical preferred frame. This article continues an examination\n",
      "of effects on the motion of binary pulsar systems in this theory, by\n",
      "incorporating effects due to strong fields in the vicinity of neutron star\n",
      "pulsars. These effects are included through an effective approach, by treating\n",
      "the compact bodies as point particles with nonstandard, velocity dependent\n",
      "interactions parametrized by dimensionless \"sensitivit ...\n",
      "\n",
      "REAL CATEGORY: gr-qc\n",
      "PREDICTED CATEGORY: hep-th\n"
     ]
    }
   ],
   "source": [
    "true_label = example[\"label\"]\n",
    "pred_index = int(example[\"prediction\"])\n",
    "pred_label = labels[pred_index]\n",
    "\n",
    "print(\"ABSTRACT:\")\n",
    "print(example[\"abstract\"][:500], \"...\")\n",
    "\n",
    "print(\"\\nREAL CATEGORY:\", true_label)\n",
    "print(\"PREDICTED CATEGORY:\", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb5e0ac-6b57-4511-a01e-de7352efe93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]\n",
      "PySpark version: 3.5.0\n",
      "Spark NLP version: 5.5.0\n",
      "Scala version in use: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark==3.5.0\n",
    "# !pip uninstall pyspark spark-nlp -y\n",
    "# import pyspark\n",
    "# import sparknlp\n",
    "# import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "print(f\"Scala version in use: {pyspark.sql.SparkSession.builder.getOrCreate().version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72a15bf-9176-4811-add8-69f28a472724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 13:22:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m df_ml \u001b[38;5;241m=\u001b[39m df_ml\u001b[38;5;241m.\u001b[39mjoin(top_labels, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     27\u001b[0m              \u001b[38;5;241m.\u001b[39mselect(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m train, test \u001b[38;5;241m=\u001b[39m df_ml\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[43mDocumentAssembler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m distilbert \u001b[38;5;241m=\u001b[39m DistilBertEmbeddings\u001b[38;5;241m.\u001b[39mpretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert_base_uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sparknlp/base/document_assembler.py:96\u001b[0m, in \u001b[0;36mDocumentAssembler.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDocumentAssembler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.DocumentAssembler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, cleanupMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sparknlp/internal/annotator_transformer.py:36\u001b[0m, in \u001b[0;36mAnnotatorTransformer.__init__\u001b[0;34m(self, classname)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "# top_labels = df_ml.groupBy(\"label\") \\\n",
    "#     .count() \\\n",
    "#     .orderBy(col(\"count\").desc()) \\\n",
    "#     .limit(5)\n",
    "\n",
    "# df_ml = df_ml.join(top_labels, \"label\") \\\n",
    "#              .select(\"text\", \"label\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DistilBertEmbeddings, ClassifierDLApproach\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = sparknlp.start()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Arxiv_BERT\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "    .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "    .na.drop()\n",
    "\n",
    "top_labels = df_ml.groupBy(\"label\").count().orderBy(col(\"count\").desc()).limit(5)\n",
    "df_ml = df_ml.join(top_labels, \"label\") \\\n",
    "             .select(col(\"abstract\").alias(\"text\"), \"label\")\n",
    "\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "document = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "distilbert = DistilBertEmbeddings.pretrained(\"distilbert_base_uncased\",\"en\") \\\n",
    "    .setInputCols([\"document\",\"token\"]).setOutputCol(\"embeddings\")\n",
    "classifier = ClassifierDLApproach().setInputCols([\"embeddings\"]) \\\n",
    "    .setOutputCol(\"prediction\").setLabelColumn(\"label\") \\\n",
    "    .setBatchSize(8).setMaxEpochs(3).setEnableOutputLogs(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[document, tokenizer, distilbert, classifier])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions.select(\"text\", \"label\", \"prediction.result\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda79ef-07e0-41fa-ba8e-28b17454f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "    .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "    .na.drop()\n",
    "\n",
    "top_labels = df_ml.groupBy(\"label\").count().orderBy(col(\"count\").desc()).limit(5)\n",
    "df_ml = df_ml.join(top_labels, \"label\").select(col(\"abstract\").alias(\"text\"), \"label\")\n",
    "\n",
    "# df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "#     .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "#     .select(\n",
    "#         col(\"abstract\").alias(\"text\"),\n",
    "#         \"label\"\n",
    "#     ) \\\n",
    "#     .na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f19f57-5389-450d-a763-bfb0d62d090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = df_ml.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221f72d-7bbf-4204-8253-e7d4710968c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ml.columns)\n",
    "print(sparknlp.version())\n",
    "import sparknlp\n",
    "print(\"Spark NLP version:\", sparknlp.version())\n",
    "print(\"Spark NLP location:\", sparknlp.__file__)\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0064a3-c958-4bf7-a038-39ef111e7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import ClassifierDLApproach, DistilBertEmbeddings, Tokenizer\n",
    "from sparknlp.base import DocumentAssembler\n",
    "document = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "distilbert = DistilBertEmbeddings.pretrained(\n",
    "    \"distilbert_base_uncased\", \"en\"\n",
    ").setInputCols([\"document\", \"token\"]) \\\n",
    " .setOutputCol(\"embeddings\")\n",
    "\n",
    "classifier = ClassifierDLApproach() \\\n",
    "    .setInputCols([\"embeddings\"]) \\\n",
    "    .setOutputCol(\"prediction\") \\\n",
    "    .setLabelColumn(\"label\") \\\n",
    "    .setBatchSize(8) \\\n",
    "    .setMaxEpochs(3) \\\n",
    "    .setEnableOutputLogs(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document,\n",
    "    tokenizer,\n",
    "    distilbert,\n",
    "    classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00f380-b0ad-4942-a219-46c5b4dbbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8cbdd-7231-4a81-9387-09d6721a5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e70a3-3b75-48ff-9fcf-291173916bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()\n",
    "train.show(1)\n",
    "print(\"Train columns:\", train.columns)\n",
    "print(\"Test columns:\", test.columns)\n",
    "train.show(1, truncate=False)\n",
    "test.show(1, truncate=False)\n",
    "\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadd3ef-4a2f-48c9-94f4-0c02379b328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.withColumn(\n",
    "    \"predicted_label\",\n",
    "    col(\"prediction.result\")[0]\n",
    ")\n",
    "\n",
    "predictions.select(\n",
    "    \"text\",\n",
    "    \"label\",\n",
    "    \"predicted_label\"\n",
    ").show(5, truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce660a-c09a-421c-a8a7-21b39e150c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"label\",\n",
    "    outputCol=\"labelIndex\"\n",
    ").fit(predictions)\n",
    "\n",
    "pred_indexer = StringIndexer(\n",
    "    inputCol=\"predicted_label\",\n",
    "    outputCol=\"predictionIndex\"\n",
    ").fit(predictions)\n",
    "\n",
    "eval_df = label_indexer.transform(predictions)\n",
    "eval_df = pred_indexer.transform(eval_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\",\n",
    "    predictionCol=\"predictionIndex\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f1 = evaluator.evaluate(eval_df)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71fbbb-fe66-4749-b2d5-a86928a48864",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = eval_df.groupBy(\n",
    "    \"labelIndex\",\n",
    "    \"predictionIndex\"\n",
    ").count().orderBy(\"labelIndex\", \"predictionIndex\")\n",
    "\n",
    "confusion.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88364d54-14b3-4bcc-9c76-0c54c9f25c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = predictions.select(\n",
    "    \"text\",\n",
    "    \"label\",\n",
    "    \"predicted_label\"\n",
    ").limit(1).collect()[0]\n",
    "\n",
    "print(\"\\nABSTRACT:\\n\", example[\"text\"][:500], \"...\\n\")\n",
    "print(\"REAL LABEL:\", example[\"label\"])\n",
    "print(\"PREDICTED LABEL:\", example[\"predicted_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c9631-6ac4-46fd-913d-e037b482aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"arxiv_distilbert_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd54bf-6baf-4072-8b02-e31ed881f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8e6f6a862556b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:45:02.310163Z",
     "start_time": "2025-12-18T13:45:01.029763Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = []\n",
    "# with open('arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         data.append(json.loads(line))\n",
    "#         # if i >= 49999:\n",
    "#         #     break\n",
    "\n",
    "# print(f\"Loaded {len(data)} articles\")\n",
    "data = []\n",
    "corrupted_lines = 0\n",
    "\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        # curățare caractere problematice comune\n",
    "        line = line.replace(\"\\x00\", \"\")      # caractere null\n",
    "        line = line.replace(\"\\r\\n\", \"\")      # newline Windows\n",
    "        line = line.replace(\"\\n\", \"\")        # newline UNIX\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "            data.append(paper)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                # încercare reparare simplă: escape backslash și ghilimele\n",
    "                line = line.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", '\"')\n",
    "                paper = json.loads(line)\n",
    "                data.append(paper)\n",
    "            except json.JSONDecodeError:\n",
    "                corrupted_lines += 1\n",
    "                continue  # sărim linia coruptă\n",
    "\n",
    "print(f\"Număr linii corupte ignorate: {corrupted_lines}\")\n",
    "print(f\"Număr linii încărcate: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13221d73abebc77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:45:48.177272Z",
     "start_time": "2025-12-18T13:45:46.614081Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDimensions: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nAvailable columns:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68f1187f941357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:46:25.712110Z",
     "start_time": "2025-12-18T13:46:25.067140Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIRST 3 ARTICLES\")\n",
    "print(\"=\"*60)\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Columns': missing.index,\n",
    "    'Missing Values': missing.values,\n",
    "    'Percent (%)': missing_pct.values\n",
    "})\n",
    "print(missing_df[missing_df['Missing Values'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f48f4efdfa3009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:47:22.466443Z",
     "start_time": "2025-12-18T13:47:22.205040Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"CATEGORY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['primary_category'] = df['categories'].str.split().str[0]\n",
    "\n",
    "print(f\"\\nTotal unique categories: {df['primary_category'].nunique()}\")\n",
    "print(f\"\\nTop 15 most popular categories:\")\n",
    "print(df['primary_category'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e92799fe908e5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:48:29.189712Z",
     "start_time": "2025-12-18T13:48:29.099492Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEMPORAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['update_date'] = pd.to_datetime(df['update_date'])\n",
    "df['year'] = df['update_date'].dt.year\n",
    "df['month'] = df['update_date'].dt.month\n",
    "\n",
    "print(f\"\\nPeriod covered: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"\\nArticles per year (last 10 years):\")\n",
    "print(df['year'].value_counts().sort_index().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706091c9a0d59e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:49:08.590461Z",
     "start_time": "2025-12-18T13:49:07.910267Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUTHOR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['num_authors'] = df['authors'].str.split(',').str.len()\n",
    "print(f\"\\nAuthor count statistics per article:\")\n",
    "print(df['num_authors'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d72cb163b3eaa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:50:03.988738Z",
     "start_time": "2025-12-18T13:50:03.953741Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ABSTRACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['abstract_length'] = df['abstract'].str.len()\n",
    "print(f\"\\nAbstract length statistics (characters):\")\n",
    "print(df['abstract_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb028880ade8ffd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:50:54.073054Z",
     "start_time": "2025-12-18T13:50:52.926588Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "top_cats = df['primary_category'].value_counts().head(15)\n",
    "axes[0, 0].barh(range(len(top_cats)), top_cats.values, color='steelblue')\n",
    "axes[0, 0].set_yticks(range(len(top_cats)))\n",
    "axes[0, 0].set_yticklabels(top_cats.index)\n",
    "axes[0, 0].set_xlabel('Number of Articles')\n",
    "axes[0, 0].set_title('Top 15 Most Popular Categories')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "yearly_counts = df['year'].value_counts().sort_index()\n",
    "axes[0, 1].plot(yearly_counts.index, yearly_counts.values, marker='o', linewidth=2, color='darkgreen')\n",
    "axes[0, 1].set_xlabel('Year')\n",
    "axes[0, 1].set_ylabel('Number of Articles')\n",
    "axes[0, 1].set_title('Publication Evolution Over Time')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(df['num_authors'].dropna(), bins=50, color='coral', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Number of Authors')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Authors per Article')\n",
    "axes[1, 0].set_xlim(0, 20)\n",
    "\n",
    "axes[1, 1].hist(df['abstract_length'].dropna(), bins=50, color='mediumpurple', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Abstract Length (characters)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Abstract Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('arxiv_initial_exploration.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Visualizations saved to 'arxiv_initial_exploration.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd1e9fb7ce1f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:52:43.471789Z",
     "start_time": "2025-12-18T13:52:43.032046Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle('arxiv_sample_processed.pkl')\n",
    "print(\"Processed dataset saved to 'arxiv_sample_processed.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb975eca57ccac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:27:21.942554Z",
     "start_time": "2026-01-08T13:26:57.033102Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "bad_lines = 0\n",
    "\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            bad_lines += 1\n",
    "            continue\n",
    "\n",
    "        if \"categories\" in paper and paper[\"categories\"]:\n",
    "            counter[paper[\"categories\"].split()[0]] += 1\n",
    "\n",
    "print(f\"Skipped {bad_lines} malformed lines\")\n",
    "print(\"\\nTop 20 most popular categories:\")\n",
    "print(counter.most_common(20))\n",
    "\n",
    "top_categories = {k for k, _ in counter.most_common(20)}\n",
    "\n",
    "print(f\"\\nTop 20 most popular categories:\", top_categories)\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c6c29f95060a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:50:06.285397Z",
     "start_time": "2026-01-08T13:49:42.029326Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_PER_CATEGORY = 1000\n",
    "data = []\n",
    "counts = defaultdict(int)\n",
    "bad_lines = 0\n",
    "\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            bad_lines += 1\n",
    "            continue\n",
    "\n",
    "        label = paper[\"categories\"].split()[0]\n",
    "\n",
    "        if label in top_categories and counts[label] < MAX_PER_CATEGORY:\n",
    "            data.append(paper)\n",
    "            counts[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0421077ae6e7a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:05:34.516415Z",
     "start_time": "2026-01-08T14:05:34.436962Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "# print(df.head(10))\n",
    "print(df[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d345311dd70f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:05:59.977812Z",
     "start_time": "2026-01-08T14:05:59.973312Z"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORY_FULL_NAME = {\n",
    "    \"math.PR\": \"Mathematics - Probability\",\n",
    "    \"quant-ph\": \"Quantum Physics\",\n",
    "    \"math.AG\": \"Mathematics - Algebraic Geometry\",\n",
    "    \"cs.CV\": \"Computer Science - Computer Vision\",\n",
    "    \"cond-mat.mtrl-sci\": \"Condensed Matter - Materials Science\",\n",
    "    \"cond-mat.mes-hall\": \"Condensed Matter - Mesoscale and Nanoscale Physics\",\n",
    "    \"astro-ph.CO\": \"Astrophysics - Cosmology\",\n",
    "    \"math.CO\": \"Mathematics - Combinatorics\",\n",
    "    \"hep-ph\": \"High Energy Physics - Phenomenology\",\n",
    "    \"cond-mat.str-el\": \"Condensed Matter - Strongly Correlated Electrons\",\n",
    "    \"hep-th\": \"High Energy Physics - Theory\",\n",
    "    \"cs.LG\": \"Computer Science - Machine Learning\",\n",
    "    \"astro-ph\": \"Astrophysics\",\n",
    "    \"gr-qc\": \"General Relativity and Quantum Cosmology\",\n",
    "    \"cond-mat.stat-mech\": \"Condensed Matter - Statistical Mechanics\",\n",
    "    \"astro-ph.GA\": \"Astrophysics - Galaxy Astrophysics\",\n",
    "    \"astro-ph.SR\": \"Astrophysics - Solar and Stellar Astrophysics\",\n",
    "    \"cs.CL\": \"Computer Science - Computation and Language\",\n",
    "    \"math.AP\": \"Mathematics - Analysis of PDEs\",\n",
    "    \"astro-ph.HE\": \"Astrophysics - High Energy Astrophysics\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f71aba4062eba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:06:02.860269Z",
     "start_time": "2026-01-08T14:06:02.833390Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_full_names(categories_str):\n",
    "    codes = categories_str.split(\" \")\n",
    "    mapped = [CATEGORY_FULL_NAME.get(code.strip(), code.strip()) for code in codes]\n",
    "    return mapped\n",
    "\n",
    "df[\"full_names_list\"] = df[\"categories\"].apply(map_full_names)\n",
    "df[\"label\"] = df[\"full_names_list\"].apply(lambda x: x[0] if len(x) > 0 else \"Unknown\")\n",
    "print(df[[\"label\"]].value_counts().head(20))\n",
    "print(df[[\"label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c6bf0d3ef3920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:06:07.854631Z",
     "start_time": "2026-01-08T14:06:07.356410Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\$.*?\\$\", \" \", text)\n",
    "    text = re.sub(r\"\\\\[a-zA-Z]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"clean_abstract\"] = df[\"abstract\"].fillna(\"\").apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8fd6b80a13773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:57:47.025438Z",
     "start_time": "2026-01-08T13:57:46.916924Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"clean_abstract\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514cd6a490690d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:57:59.826318Z",
     "start_time": "2026-01-08T13:57:59.802683Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=20000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "        min_df=5\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c6d09ba7aada3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:58:11.456360Z",
     "start_time": "2026-01-08T13:58:03.725797Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9c1cf2a444dce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:58:15.065469Z",
     "start_time": "2026-01-08T13:58:14.673947Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf96705a796405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:58:29.389163Z",
     "start_time": "2026-01-08T13:58:29.127278Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "labels = sorted(y.unique())\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, xticklabels=labels, yticklabels=labels, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306297dca411aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:58:39.822084Z",
     "start_time": "2026-01-08T13:58:39.814223Z"
    }
   },
   "outputs": [],
   "source": [
    "test_abstract = \"\"\"\n",
    "We study a neural network architecture for solving partial differential\n",
    "equations arising in fluid dynamics.\n",
    "\"\"\"\n",
    "\n",
    "print(pipeline.predict([clean_text(test_abstract)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3c60daaf939ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:06:17.345835Z",
     "start_time": "2026-01-08T14:06:17.342669Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfa09dc2f5527e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:08:24.348363Z",
     "start_time": "2026-01-08T14:08:24.334794Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df[[\"clean_abstract\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45198573cad6e909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:08:36.563907Z",
     "start_time": "2026-01-08T14:08:36.369292Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"your_cleaned_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5964d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d23c325cf87f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T15:22:06.056068Z",
     "start_time": "2026-01-08T15:21:47.815063Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"your_cleaned_dataframe.csv\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"label\"])\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"label_id\"]\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"clean_abstract\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label_id\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label_id\"])\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./scibert_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "model.save_pretrained(\"./scibert_model\")\n",
    "tokenizer.save_pretrained(\"./scibert_model\")\n",
    "import joblib\n",
    "\n",
    "joblib.dump(le, \"./scibert_model/label_encoder.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
