{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd671ea",
   "metadata": {},
   "source": [
    "\n",
    "# arXiv Classification with Spark NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ad6504-8a02-46cd-b3c0-fc8a7b61bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyspark 3.5.0\n",
      "Uninstalling pyspark-3.5.0:\n",
      "  Successfully uninstalled pyspark-3.5.0\n",
      "Found existing installation: spark-nlp 5.5.0\n",
      "Uninstalling spark-nlp-5.5.0:\n",
      "  Successfully uninstalled spark-nlp-5.5.0\n",
      "Files removed: 6 (1.4 MB)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.5.0\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425400 sha256=e823914c1d9981c3a4b38ddc48722de55d088deaab4a5ad9f863e1124ddf5022\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7h97zyba/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spark-nlp==5.5.0\n",
      "  Downloading spark_nlp-5.5.0-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Downloading spark_nlp-5.5.0-py2.py3-none-any.whl (620 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.8/620.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.5.0\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall pyspark spark-nlp -y\n",
    "# !pip uninstall pyspark spark-nlp -y  # Da, de 2 ori pentru siguranță\n",
    "\n",
    "# !pip cache purge\n",
    "\n",
    "# !pip install --no-cache-dir --force-reinstall pyspark==3.5.0\n",
    "# !pip install --no-cache-dir --force-reinstall spark-nlp==5.5.0\n",
    "!pip uninstall pyspark spark-nlp -y\n",
    "!rm -rf ~/.ivy2/jars/*\n",
    "!rm -rf ~/.ivy2/cache/com.johnsnowlabs.nlp/\n",
    "!pip cache purge\n",
    "\n",
    "!pip install --no-cache-dir pyspark==3.5.0\n",
    "!pip install --no-cache-dir spark-nlp==5.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c156198-54be-4fb0-9b96-5e76b8e25b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e5c80b9a-fd51-4d66-8b20-0d65374780a7;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.5.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.19.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.1-rc2 in central\n",
      "\tfound org.jetbrains#annotations;24.1.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp_2.12/5.5.0/spark-nlp_2.12-5.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#spark-nlp_2.12;5.5.0!spark-nlp_2.12.jar (761ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/tensorflow-cpu_2.12/0.4.4/tensorflow-cpu_2.12-0.4.4.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4!tensorflow-cpu_2.12.jar (1267ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/jsl-llamacpp-cpu_2.12/0.1.1-rc2/jsl-llamacpp-cpu_2.12-0.1.1-rc2.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.1-rc2!jsl-llamacpp-cpu_2.12.jar (64ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/jsl-openvino-cpu_2.12/0.1.0/jsl-openvino-cpu_2.12-0.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0!jsl-openvino-cpu_2.12.jar (291ms)\n",
      ":: resolution report :: resolve 2955ms :: artifacts dl 2436ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.1-rc2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.5.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.19.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.jetbrains#annotations;24.1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   86  |   4   |   4   |   5   ||   81  |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e5c80b9a-fd51-4d66-8b20-0d65374780a7\n",
      "\tconfs: [default]\n",
      "\t81 artifacts copied, 0 already retrieved (596927kB/7730ms)\n",
      "26/01/09 15:00:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark version: 3.5.0\n",
      "✓ Spark NLP version: 5.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Configurare care forțează descărcarea JAR-ului 5.5.0\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.0\")\n",
    "conf.set(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\n",
    "conf.set(\"spark.driver.memory\", \"16g\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Arxiv_BERT\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark version: {spark.version}\")\n",
    "\n",
    "import sparknlp\n",
    "print(f\"✓ Spark NLP version: {sparknlp.version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861a3363-3252-4b6e-a2a2-40f3a2f5f287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]\n",
      "PySpark version: 3.5.0\n",
      "Spark NLP version: 5.5.0\n",
      "Spark runtime version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import sparknlp\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")  \n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(f\"Spark runtime version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b05c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "Spark NLP initialized\n",
      "Spark version: 3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 15:00:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start(\n",
    "    memory=\"6g\"\n",
    ")\n",
    "\n",
    "print(\"Spark NLP initialized\")\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca025da2-439f-4dc4-91d8-f9d78e953b7d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.10/site-packages (25.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.10/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /home/ubuntu/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/.local/lib/python3.10/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /home/ubuntu/.local/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch) (59.6.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Invalid requirement: '[torch]': Expected package name at the start of dependency specifier\n",
      "    [torch]\n",
      "    ^\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sparknlp in /home/ubuntu/.local/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: spark-nlp in /home/ubuntu/.local/lib/python3.10/site-packages (from sparknlp) (5.5.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from sparknlp) (2.2.6)\n",
      "Found existing installation: pyspark 3.5.0\n",
      "Uninstalling pyspark-3.5.0:\n",
      "  Successfully uninstalled pyspark-3.5.0\n",
      "Found existing installation: spark-nlp 5.5.0\n",
      "Uninstalling spark-nlp-5.5.0:\n",
      "  Successfully uninstalled spark-nlp-5.5.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.5.0\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas matplotlib seaborn numpy scikit-learn\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers\n",
    "!pip install transformers [torch]\n",
    "!pip install sparknlp\n",
    "!pip uninstall -y pyspark spark-nlp\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install spark-nlp==5.5.0\n",
    "!pip install datasets\n",
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DistilBertEmbeddings, ClassifierDLApproach\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T11:52:12.513859Z",
     "start_time": "2026-01-08T11:52:12.508662Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sparknlp\n",
    "import warnings\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad1a98-ef16-4f96-9ffa-46a98bb4a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spark NLP initialized\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f127f-f03a-4f12-b5ba-baab0f67c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, StopWordsRemover, StringIndexer, Tokenizer\n",
    "from pyspark.sql.functions import col, split, explode, count\n",
    "\n",
    "from sparknlp.annotator import ClassifierDLApproach, DistilBertEmbeddings, Tokenizer\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    Tokenizer,\n",
    "    StopWordsRemover,\n",
    "    HashingTF,\n",
    "    IDF\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48975aa7-659f-42b3-9c81-5ddb6b7f95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "df.select(\"categories\").show(5, truncate=False)\n",
    "\n",
    "unique_count = df.select(\"categories\").distinct().count()\n",
    "print(f\"Unique categories count: {unique_count}\")\n",
    "\n",
    "top_categories = df.groupBy(\"categories\") \\\n",
    "                   .agg(count(\"*\").alias(\"count\")) \\\n",
    "                   .orderBy(col(\"count\").desc()) \\\n",
    "                   .limit(20)\n",
    "\n",
    "top_categories.show(truncate=False)\n",
    "\n",
    "df_exploded = df.withColumn(\"category\", explode(split(col(\"categories\"), \" \")))\n",
    "\n",
    "unique_exploded = df_exploded.select(\"category\").distinct().count()\n",
    "print(f\"Unique categories after split: {unique_exploded}\")\n",
    "\n",
    "df_exploded.groupBy(\"category\") \\\n",
    "           .agg(count(\"*\").alias(\"count\")) \\\n",
    "           .orderBy(col(\"count\").desc()) \\\n",
    "           .show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684290b7-a5ec-42eb-9754-00ea52adb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "          .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "          .select(\"abstract\", \"label\") \\\n",
    "          .na.drop()\n",
    "\n",
    "df_ml.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f421e-1a90-4c7c-976a-3ccb857a31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"label\",\n",
    "    outputCol=\"labelIndex\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    inputCol=\"abstract\",\n",
    "    outputCol=\"words\"\n",
    ")\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"words\",\n",
    "    outputCol=\"filtered_words\"\n",
    ")\n",
    "\n",
    "hashingTF = HashingTF(\n",
    "    inputCol=\"filtered_words\",\n",
    "    outputCol=\"rawFeatures\",\n",
    "    numFeatures=20000\n",
    ")\n",
    "\n",
    "idf = IDF(\n",
    "    inputCol=\"rawFeatures\",\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"labelIndex\",\n",
    "    maxIter=20\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    hashingTF,\n",
    "    idf,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10020f-acc1-440f-83dc-8e6c4a65316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions.select(\"abstract\", \"label\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc371b-c4a8-403a-aa34-8868ec8f52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a64762-4b85-48ae-ba11-b2c4ef6ae388",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = predictions.groupBy(\"labelIndex\", \"prediction\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"labelIndex\", \"prediction\")\n",
    "\n",
    "confusion.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1fcc6-2661-4635-91cc-2e6cc91dd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.stages[0].labels  # din StringIndexer\n",
    "for i, label in enumerate(labels):\n",
    "    print(i, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1e600-4bc1-4fc3-8aa1-821da20d1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_weighted = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "print(\"F1 weighted:\", f1_weighted.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dcc2a-06ec-4c28-8268-36a550896bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = predictions.select(\n",
    "    \"abstract\",\n",
    "    \"label\",\n",
    "    \"prediction\"\n",
    ").limit(1).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ea4c3-d064-4796-8b1b-fe060ce9e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = example[\"label\"]\n",
    "pred_index = int(example[\"prediction\"])\n",
    "pred_label = labels[pred_index]\n",
    "\n",
    "print(\"ABSTRACT:\")\n",
    "print(example[\"abstract\"][:500], \"...\")\n",
    "\n",
    "print(\"\\nREAL CATEGORY:\", true_label)\n",
    "print(\"PREDICTED CATEGORY:\", pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5e0ac-6b57-4511-a01e-de7352efe93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark==3.5.0\n",
    "# !pip uninstall pyspark spark-nlp -y\n",
    "# import pyspark\n",
    "# import sparknlp\n",
    "# import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "print(f\"Scala version in use: {pyspark.sql.SparkSession.builder.getOrCreate().version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a15bf-9176-4811-add8-69f28a472724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_labels = df_ml.groupBy(\"label\") \\\n",
    "#     .count() \\\n",
    "#     .orderBy(col(\"count\").desc()) \\\n",
    "#     .limit(5)\n",
    "\n",
    "# df_ml = df_ml.join(top_labels, \"label\") \\\n",
    "#              .select(\"text\", \"label\")\n",
    "import sparknlp\n",
    "\n",
    "# 2. APOI: Importă și folosește annotatoarele\n",
    "from pyspark.sql.functions import col, split\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DistilBertEmbeddings, ClassifierDLApproach\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, split\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DistilBertEmbeddings, ClassifierDLApproach\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Configurare care forțează descărcarea JAR-ului 5.5.0\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.0\")\n",
    "conf.set(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\n",
    "conf.set(\"spark.driver.memory\", \"16g\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Arxiv_BERT\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Aceasta încarcă JAR-urile necesare\n",
    "# spark = sparknlp.start()\n",
    "\n",
    "print(f\"✓ Spark version: {spark.version}\")\n",
    "print(f\"✓ Spark NLP version: {sparknlp.version()}\")\n",
    "#spark = sparknlp.start()\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Arxiv_BERT\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "    .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "    .na.drop()\n",
    "\n",
    "top_labels = df_ml.groupBy(\"label\").count().orderBy(col(\"count\").desc()).limit(5)\n",
    "df_ml = df_ml.join(top_labels, \"label\") \\\n",
    "             .select(col(\"abstract\").alias(\"text\"), \"label\")\n",
    "\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "document = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "distilbert = DistilBertEmbeddings.pretrained(\"distilbert_base_uncased\",\"en\") \\\n",
    "    .setInputCols([\"document\",\"token\"]).setOutputCol(\"embeddings\")\n",
    "classifier = ClassifierDLApproach().setInputCols([\"embeddings\"]) \\\n",
    "    .setOutputCol(\"prediction\").setLabelColumn(\"label\") \\\n",
    "    .setBatchSize(8).setMaxEpochs(3).setEnableOutputLogs(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[document, tokenizer, distilbert, classifier])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions.select(\"text\", \"label\", \"prediction.result\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda79ef-07e0-41fa-ba8e-28b17454f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "    .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "    .na.drop()\n",
    "\n",
    "top_labels = df_ml.groupBy(\"label\").count().orderBy(col(\"count\").desc()).limit(5)\n",
    "df_ml = df_ml.join(top_labels, \"label\").select(col(\"abstract\").alias(\"text\"), \"label\")\n",
    "\n",
    "# df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "#     .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "#     .select(\n",
    "#         col(\"abstract\").alias(\"text\"),\n",
    "#         \"label\"\n",
    "#     ) \\\n",
    "#     .na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f19f57-5389-450d-a763-bfb0d62d090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = df_ml.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221f72d-7bbf-4204-8253-e7d4710968c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ml.columns)\n",
    "print(sparknlp.version())\n",
    "import sparknlp\n",
    "print(\"Spark NLP version:\", sparknlp.version())\n",
    "print(\"Spark NLP location:\", sparknlp.__file__)\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0064a3-c958-4bf7-a038-39ef111e7bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import ClassifierDLApproach, DistilBertEmbeddings, Tokenizer\n",
    "from pyspark.sql.functions import col, split\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, DistilBertEmbeddings, ClassifierDLApproach\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Aceasta încarcă JAR-urile necesare\n",
    "# spark = sparknlp.start()\n",
    "\n",
    "print(f\"✓ Spark version: {spark.version}\")\n",
    "print(f\"✓ Spark NLP version: {sparknlp.version()}\")\n",
    "\n",
    "document = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "distilbert = DistilBertEmbeddings.pretrained(\n",
    "    \"distilbert_base_uncased\", \"en\"\n",
    ").setInputCols([\"document\", \"token\"]) \\\n",
    " .setOutputCol(\"embeddings\")\n",
    "\n",
    "classifier = ClassifierDLApproach() \\\n",
    "    .setInputCols([\"embeddings\"]) \\\n",
    "    .setOutputCol(\"prediction\") \\\n",
    "    .setLabelColumn(\"label\") \\\n",
    "    .setBatchSize(8) \\\n",
    "    .setMaxEpochs(3) \\\n",
    "    .setEnableOutputLogs(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document,\n",
    "    tokenizer,\n",
    "    distilbert,\n",
    "    classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00f380-b0ad-4942-a219-46c5b4dbbe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8cbdd-7231-4a81-9387-09d6721a5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e70a3-3b75-48ff-9fcf-291173916bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()\n",
    "train.show(1)\n",
    "print(\"Train columns:\", train.columns)\n",
    "print(\"Test columns:\", test.columns)\n",
    "train.show(1, truncate=False)\n",
    "test.show(1, truncate=False)\n",
    "\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadd3ef-4a2f-48c9-94f4-0c02379b328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.withColumn(\n",
    "    \"predicted_label\",\n",
    "    col(\"prediction.result\")[0]\n",
    ")\n",
    "\n",
    "predictions.select(\n",
    "    \"text\",\n",
    "    \"label\",\n",
    "    \"predicted_label\"\n",
    ").show(5, truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce660a-c09a-421c-a8a7-21b39e150c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"label\",\n",
    "    outputCol=\"labelIndex\"\n",
    ").fit(predictions)\n",
    "\n",
    "pred_indexer = StringIndexer(\n",
    "    inputCol=\"predicted_label\",\n",
    "    outputCol=\"predictionIndex\"\n",
    ").fit(predictions)\n",
    "\n",
    "eval_df = label_indexer.transform(predictions)\n",
    "eval_df = pred_indexer.transform(eval_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\",\n",
    "    predictionCol=\"predictionIndex\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f1 = evaluator.evaluate(eval_df)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71fbbb-fe66-4749-b2d5-a86928a48864",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = eval_df.groupBy(\n",
    "    \"labelIndex\",\n",
    "    \"predictionIndex\"\n",
    ").count().orderBy(\"labelIndex\", \"predictionIndex\")\n",
    "\n",
    "confusion.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88364d54-14b3-4bcc-9c76-0c54c9f25c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = predictions.select(\n",
    "    \"text\",\n",
    "    \"label\",\n",
    "    \"predicted_label\"\n",
    ").limit(1).collect()[0]\n",
    "\n",
    "print(\"\\nABSTRACT:\\n\", example[\"text\"][:500], \"...\\n\")\n",
    "print(\"REAL LABEL:\", example[\"label\"])\n",
    "print(\"PREDICTED LABEL:\", example[\"predicted_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c9631-6ac4-46fd-913d-e037b482aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"arxiv_distilbert_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd54bf-6baf-4072-8b02-e31ed881f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8e6f6a862556b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:45:02.310163Z",
     "start_time": "2025-12-18T13:45:01.029763Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = []\n",
    "# with open('arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         data.append(json.loads(line))\n",
    "#         # if i >= 49999:\n",
    "#         #     break\n",
    "\n",
    "# print(f\"Loaded {len(data)} articles\")\n",
    "data = []\n",
    "corrupted_lines = 0\n",
    "\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        # curățare caractere problematice comune\n",
    "        line = line.replace(\"\\x00\", \"\")      # caractere null\n",
    "        line = line.replace(\"\\r\\n\", \"\")      # newline Windows\n",
    "        line = line.replace(\"\\n\", \"\")        # newline UNIX\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "            data.append(paper)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                # încercare reparare simplă: escape backslash și ghilimele\n",
    "                line = line.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", '\"')\n",
    "                paper = json.loads(line)\n",
    "                data.append(paper)\n",
    "            except json.JSONDecodeError:\n",
    "                corrupted_lines += 1\n",
    "                continue  # sărim linia coruptă\n",
    "\n",
    "print(f\"Număr linii corupte ignorate: {corrupted_lines}\")\n",
    "print(f\"Număr linii încărcate: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13221d73abebc77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:45:48.177272Z",
     "start_time": "2025-12-18T13:45:46.614081Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDimensions: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nAvailable columns:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68f1187f941357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:46:25.712110Z",
     "start_time": "2025-12-18T13:46:25.067140Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIRST 3 ARTICLES\")\n",
    "print(\"=\"*60)\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Columns': missing.index,\n",
    "    'Missing Values': missing.values,\n",
    "    'Percent (%)': missing_pct.values\n",
    "})\n",
    "print(missing_df[missing_df['Missing Values'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f48f4efdfa3009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:47:22.466443Z",
     "start_time": "2025-12-18T13:47:22.205040Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"CATEGORY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['primary_category'] = df['categories'].str.split().str[0]\n",
    "\n",
    "print(f\"\\nTotal unique categories: {df['primary_category'].nunique()}\")\n",
    "print(f\"\\nTop 15 most popular categories:\")\n",
    "print(df['primary_category'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e92799fe908e5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:48:29.189712Z",
     "start_time": "2025-12-18T13:48:29.099492Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEMPORAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['update_date'] = pd.to_datetime(df['update_date'])\n",
    "df['year'] = df['update_date'].dt.year\n",
    "df['month'] = df['update_date'].dt.month\n",
    "\n",
    "print(f\"\\nPeriod covered: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"\\nArticles per year (last 10 years):\")\n",
    "print(df['year'].value_counts().sort_index().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706091c9a0d59e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:49:08.590461Z",
     "start_time": "2025-12-18T13:49:07.910267Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AUTHOR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['num_authors'] = df['authors'].str.split(',').str.len()\n",
    "print(f\"\\nAuthor count statistics per article:\")\n",
    "print(df['num_authors'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d72cb163b3eaa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:50:03.988738Z",
     "start_time": "2025-12-18T13:50:03.953741Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ABSTRACT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['abstract_length'] = df['abstract'].str.len()\n",
    "print(f\"\\nAbstract length statistics (characters):\")\n",
    "print(df['abstract_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb028880ade8ffd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:50:54.073054Z",
     "start_time": "2025-12-18T13:50:52.926588Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "top_cats = df['primary_category'].value_counts().head(15)\n",
    "axes[0, 0].barh(range(len(top_cats)), top_cats.values, color='steelblue')\n",
    "axes[0, 0].set_yticks(range(len(top_cats)))\n",
    "axes[0, 0].set_yticklabels(top_cats.index)\n",
    "axes[0, 0].set_xlabel('Number of Articles')\n",
    "axes[0, 0].set_title('Top 15 Most Popular Categories')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "yearly_counts = df['year'].value_counts().sort_index()\n",
    "axes[0, 1].plot(yearly_counts.index, yearly_counts.values, marker='o', linewidth=2, color='darkgreen')\n",
    "axes[0, 1].set_xlabel('Year')\n",
    "axes[0, 1].set_ylabel('Number of Articles')\n",
    "axes[0, 1].set_title('Publication Evolution Over Time')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(df['num_authors'].dropna(), bins=50, color='coral', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Number of Authors')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Authors per Article')\n",
    "axes[1, 0].set_xlim(0, 20)\n",
    "\n",
    "axes[1, 1].hist(df['abstract_length'].dropna(), bins=50, color='mediumpurple', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Abstract Length (characters)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Abstract Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('arxiv_initial_exploration.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Visualizations saved to 'arxiv_initial_exploration.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd1e9fb7ce1f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T13:52:43.471789Z",
     "start_time": "2025-12-18T13:52:43.032046Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle('arxiv_sample_processed.pkl')\n",
    "print(\"Processed dataset saved to 'arxiv_sample_processed.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb975eca57ccac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:27:21.942554Z",
     "start_time": "2026-01-08T13:26:57.033102Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "bad_lines = 0\n",
    "\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            bad_lines += 1\n",
    "            continue\n",
    "\n",
    "        if \"categories\" in paper and paper[\"categories\"]:\n",
    "            counter[paper[\"categories\"].split()[0]] += 1\n",
    "\n",
    "print(f\"Skipped {bad_lines} malformed lines\")\n",
    "print(\"\\nTop 20 most popular categories:\")\n",
    "print(counter.most_common(20))\n",
    "\n",
    "top_categories = {k for k, _ in counter.most_common(20)}\n",
    "\n",
    "print(f\"\\nTop 20 most popular categories:\", top_categories)\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c6c29f95060a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:50:06.285397Z",
     "start_time": "2026-01-08T13:49:42.029326Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_PER_CATEGORY = 1000\n",
    "data = []\n",
    "counts = defaultdict(int)\n",
    "bad_lines = 0\n",
    "\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            bad_lines += 1\n",
    "            continue\n",
    "\n",
    "        label = paper[\"categories\"].split()[0]\n",
    "\n",
    "        if label in top_categories and counts[label] < MAX_PER_CATEGORY:\n",
    "            data.append(paper)\n",
    "            counts[label] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0421077ae6e7a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:05:34.516415Z",
     "start_time": "2026-01-08T14:05:34.436962Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "# print(df.head(10))\n",
    "print(df[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d345311dd70f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:05:59.977812Z",
     "start_time": "2026-01-08T14:05:59.973312Z"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORY_FULL_NAME = {\n",
    "    \"math.PR\": \"Mathematics - Probability\",\n",
    "    \"quant-ph\": \"Quantum Physics\",\n",
    "    \"math.AG\": \"Mathematics - Algebraic Geometry\",\n",
    "    \"cs.CV\": \"Computer Science - Computer Vision\",\n",
    "    \"cond-mat.mtrl-sci\": \"Condensed Matter - Materials Science\",\n",
    "    \"cond-mat.mes-hall\": \"Condensed Matter - Mesoscale and Nanoscale Physics\",\n",
    "    \"astro-ph.CO\": \"Astrophysics - Cosmology\",\n",
    "    \"math.CO\": \"Mathematics - Combinatorics\",\n",
    "    \"hep-ph\": \"High Energy Physics - Phenomenology\",\n",
    "    \"cond-mat.str-el\": \"Condensed Matter - Strongly Correlated Electrons\",\n",
    "    \"hep-th\": \"High Energy Physics - Theory\",\n",
    "    \"cs.LG\": \"Computer Science - Machine Learning\",\n",
    "    \"astro-ph\": \"Astrophysics\",\n",
    "    \"gr-qc\": \"General Relativity and Quantum Cosmology\",\n",
    "    \"cond-mat.stat-mech\": \"Condensed Matter - Statistical Mechanics\",\n",
    "    \"astro-ph.GA\": \"Astrophysics - Galaxy Astrophysics\",\n",
    "    \"astro-ph.SR\": \"Astrophysics - Solar and Stellar Astrophysics\",\n",
    "    \"cs.CL\": \"Computer Science - Computation and Language\",\n",
    "    \"math.AP\": \"Mathematics - Analysis of PDEs\",\n",
    "    \"astro-ph.HE\": \"Astrophysics - High Energy Astrophysics\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f71aba4062eba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:06:02.860269Z",
     "start_time": "2026-01-08T14:06:02.833390Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_full_names(categories_str):\n",
    "    codes = categories_str.split(\" \")\n",
    "    mapped = [CATEGORY_FULL_NAME.get(code.strip(), code.strip()) for code in codes]\n",
    "    return mapped\n",
    "\n",
    "df[\"full_names_list\"] = df[\"categories\"].apply(map_full_names)\n",
    "df[\"label\"] = df[\"full_names_list\"].apply(lambda x: x[0] if len(x) > 0 else \"Unknown\")\n",
    "print(df[[\"label\"]].value_counts().head(20))\n",
    "print(df[[\"label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c6bf0d3ef3920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:06:07.854631Z",
     "start_time": "2026-01-08T14:06:07.356410Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\$.*?\\$\", \" \", text)\n",
    "    text = re.sub(r\"\\\\[a-zA-Z]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"clean_abstract\"] = df[\"abstract\"].fillna(\"\").apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8fd6b80a13773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:57:47.025438Z",
     "start_time": "2026-01-08T13:57:46.916924Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"clean_abstract\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514cd6a490690d4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:57:59.826318Z",
     "start_time": "2026-01-08T13:57:59.802683Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=20000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "        min_df=5\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c6d09ba7aada3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:58:11.456360Z",
     "start_time": "2026-01-08T13:58:03.725797Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9c1cf2a444dce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:58:15.065469Z",
     "start_time": "2026-01-08T13:58:14.673947Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf96705a796405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:58:29.389163Z",
     "start_time": "2026-01-08T13:58:29.127278Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "labels = sorted(y.unique())\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, xticklabels=labels, yticklabels=labels, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306297dca411aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T13:58:39.822084Z",
     "start_time": "2026-01-08T13:58:39.814223Z"
    }
   },
   "outputs": [],
   "source": [
    "test_abstract = \"\"\"\n",
    "We study a neural network architecture for solving partial differential\n",
    "equations arising in fluid dynamics.\n",
    "\"\"\"\n",
    "\n",
    "print(pipeline.predict([clean_text(test_abstract)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3c60daaf939ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:06:17.345835Z",
     "start_time": "2026-01-08T14:06:17.342669Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfa09dc2f5527e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:08:24.348363Z",
     "start_time": "2026-01-08T14:08:24.334794Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df[[\"clean_abstract\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45198573cad6e909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T14:08:36.563907Z",
     "start_time": "2026-01-08T14:08:36.369292Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"your_cleaned_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5964d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d23c325cf87f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T15:22:06.056068Z",
     "start_time": "2026-01-08T15:21:47.815063Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"your_cleaned_dataframe.csv\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"label\"])\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"label_id\"]\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"clean_abstract\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label_id\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label_id\"])\n",
    "\n",
    "num_labels = len(le.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./scibert_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "model.save_pretrained(\"./scibert_model\")\n",
    "tokenizer.save_pretrained(\"./scibert_model\")\n",
    "import joblib\n",
    "\n",
    "joblib.dump(le, \"./scibert_model/label_encoder.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
