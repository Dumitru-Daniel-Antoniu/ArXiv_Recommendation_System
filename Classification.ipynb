{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd671ea",
   "metadata": {},
   "source": [
    "# arXiv Classification with Spark NLP\n",
    "\n",
    "This notebook demonstrates text classification on arXiv paper abstracts using two approaches:\n",
    "1. **TF-IDF + Logistic Regression** - Traditional ML approach\n",
    "2. **DistilBERT + ClassifierDL** - Deep learning approach with Spark NLP\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ad6504-8a02-46cd-b3c0-fc8a7b61bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyspark 3.5.0\n",
      "Uninstalling pyspark-3.5.0:\n",
      "  Successfully uninstalled pyspark-3.5.0\n",
      "Found existing installation: spark-nlp 5.5.0\n",
      "Uninstalling spark-nlp-5.5.0:\n",
      "  Successfully uninstalled spark-nlp-5.5.0\n",
      "Files removed: 6 (1.4 MB)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.5.0\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425400 sha256=c1d6c3d2b4c2290c97a24cae8631d611d70e6fc179536e943f13c706c7f13e8e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_0bmjc5v/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spark-nlp==5.5.0\n",
      "  Downloading spark_nlp-5.5.0-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Downloading spark_nlp-5.5.0-py2.py3-none-any.whl (620 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.8/620.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.5.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.10/site-packages (25.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.10/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /home/ubuntu/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/.local/lib/python3.10/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /home/ubuntu/.local/lib/python3.10/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch) (59.6.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Clean install of PySpark and Spark NLP\n",
    "!pip uninstall pyspark spark-nlp -y\n",
    "!rm -rf ~/.ivy2/jars/*\n",
    "!rm -rf ~/.ivy2/cache/com.johnsnowlabs.nlp/\n",
    "!pip cache purge\n",
    "\n",
    "!pip install --no-cache-dir pyspark==3.5.0\n",
    "!pip install --no-cache-dir spark-nlp==5.5.0\n",
    "\n",
    "# Additional dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install pandas matplotlib seaborn numpy scikit-learn\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861a3363-3252-4b6e-a2a2-40f3a2f5f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1fd87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.12 (main, Jan  8 2026, 06:52:19) [GCC 11.4.0]\n",
      "PySpark version: 3.5.0\n",
      "Spark NLP version: 5.5.0\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Data science\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, explode, count, expr\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import (\n",
    "    Tokenizer as SparkNLPTokenizer,\n",
    "    DistilBertEmbeddings,\n",
    "    ClassifierDLApproach,\n",
    "    SentenceEmbeddings\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Version check\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346aff27",
   "metadata": {},
   "source": [
    "## 3. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd0e095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7e74a884-ecf1-436b-8d51-75466fca589f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.5.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.19.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.1-rc2 in central\n",
      "\tfound org.jetbrains#annotations;24.1.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp_2.12/5.5.0/spark-nlp_2.12-5.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#spark-nlp_2.12;5.5.0!spark-nlp_2.12.jar (833ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/tensorflow-cpu_2.12/0.4.4/tensorflow-cpu_2.12-0.4.4.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4!tensorflow-cpu_2.12.jar (1696ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/jsl-llamacpp-cpu_2.12/0.1.1-rc2/jsl-llamacpp-cpu_2.12-0.1.1-rc2.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.1-rc2!jsl-llamacpp-cpu_2.12.jar (61ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/jsl-openvino-cpu_2.12/0.1.0/jsl-openvino-cpu_2.12-0.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0!jsl-openvino-cpu_2.12.jar (383ms)\n",
      ":: resolution report :: resolve 8359ms :: artifacts dl 4957ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.1-rc2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.5.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.19.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.jetbrains#annotations;24.1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   86  |   4   |   4   |   5   ||   81  |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7e74a884-ecf1-436b-8d51-75466fca589f\n",
      "\tconfs: [default]\n",
      "\t81 artifacts copied, 0 already retrieved (596927kB/17126ms)\n",
      "26/01/15 10:52:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark runtime version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark with Spark NLP\n",
    "spark = sparknlp.start(gpu=False, memory=\"8G\")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark runtime version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a38a11",
   "metadata": {},
   "source": [
    "## 4. Load & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efde4057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|categories     |\n",
      "+---------------+\n",
      "|hep-ph         |\n",
      "|math.CO cs.CG  |\n",
      "|physics.gen-ph |\n",
      "|math.CO        |\n",
      "|math.CA math.FA|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===================================>                       (3 + 2) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique category combinations: 21564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Load arXiv metadata\n",
    "df = spark.read.json(\"arxiv-metadata-oai-snapshot.json\")\n",
    "\n",
    "# Preview categories\n",
    "df.select(\"categories\").show(5, truncate=False)\n",
    "\n",
    "# Count unique category combinations\n",
    "unique_count = df.select(\"categories\").distinct().count()\n",
    "print(f\"Unique category combinations: {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ff256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|categories       |count|\n",
      "+-----------------+-----+\n",
      "|astro-ph         |16405|\n",
      "|hep-ph           |15547|\n",
      "|quant-ph         |13733|\n",
      "|astro-ph.CO      |10718|\n",
      "|hep-th           |10424|\n",
      "|astro-ph.SR      |8477 |\n",
      "|cond-mat.mes-hall|7550 |\n",
      "|cond-mat.mtrl-sci|7041 |\n",
      "|gr-qc            |5777 |\n",
      "|cond-mat.str-el  |5552 |\n",
      "|cs.IT math.IT    |5204 |\n",
      "|math.PR          |5149 |\n",
      "|math.AP          |4916 |\n",
      "|astro-ph.HE      |4872 |\n",
      "|math.CO          |4792 |\n",
      "|hep-ex           |4778 |\n",
      "|math.AG          |4211 |\n",
      "|nucl-th          |4110 |\n",
      "|cond-mat.supr-con|4026 |\n",
      "|astro-ph.GA      |3981 |\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================>                       (3 + 2) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique individual categories: 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Top 20 category combinations\n",
    "top_categories = df.groupBy(\"categories\") \\\n",
    "                   .agg(count(\"*\").alias(\"count\")) \\\n",
    "                   .orderBy(col(\"count\").desc()) \\\n",
    "                   .limit(20)\n",
    "top_categories.show(truncate=False)\n",
    "\n",
    "# Explode categories to see individual labels\n",
    "df_exploded = df.withColumn(\"category\", explode(split(col(\"categories\"), \" \")))\n",
    "unique_exploded = df_exploded.select(\"category\").distinct().count()\n",
    "print(f\"Unique individual categories: {unique_exploded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271a3753",
   "metadata": {},
   "source": [
    "## 5. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf8d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |label         |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "|  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n|hep-ph        |\n",
      "|  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\\nit obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\nreceived increased attention in recent years. In particular, our colored\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\nalso present a new decomposition that certifies sparsity based on the\\n$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\nWestermann and Hendrickson.\\n                                                                                                                                                                                            |math.CO       |\n",
      "|  The evolution of Earth-Moon system is described by the dark matter field\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\nwith this model very well and the general pattern of the evolution of the\\nMoon-Earth system described by this model agrees with geological and fossil\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\nbillion years ago, which is far beyond the Roche's limit. The result suggests\\nthat the tidal friction may not be the primary cause for the evolution of the\\nEarth-Moon system. The average dark matter field fluid constant derived from\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\nthat the Mars's rotation is also slowing with the angular acceleration rate\\nabout -4.38 x 10^(-22) rad s^(-2).\\n                                                                                                         |physics.gen-ph|\n",
      "|  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\\nsingle-source automata. The proof involves a bijection from these automata to\\ncertain marked lattice paths and a sign-reversing involution to evaluate the\\ndeterminant.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |math.CO       |\n",
      "|  In this paper we show how to compute the $\\Lambda_{\\alpha}$ norm, $\\alpha\\ge\\n0$, using the dyadic grid. This result is a consequence of the description of\\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |math.CA       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use primary category (first in list) as label\n",
    "df_ml = df.select(\"abstract\", \"categories\") \\\n",
    "          .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "          .select(\"abstract\", \"label\") \\\n",
    "          .na.drop()\n",
    "\n",
    "df_ml.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195a7fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 10 categories:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|label            |\n",
      "+-----------------+\n",
      "|hep-ph           |\n",
      "|cond-mat.mes-hall|\n",
      "|gr-qc            |\n",
      "|cond-mat.mtrl-sci|\n",
      "|astro-ph         |\n",
      "|hep-th           |\n",
      "|cond-mat.str-el  |\n",
      "|quant-ph         |\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Filter to top N categories and limit dataset size\n",
    "TOP_N = 10\n",
    "SAMPLE_SIZE = 20000\n",
    "\n",
    "top_labels = (\n",
    "    df_ml.groupBy(\"label\")\n",
    "         .agg(count(\"*\").alias(\"count\"))\n",
    "         .orderBy(col(\"count\").desc())\n",
    "         .limit(TOP_N)\n",
    "         .select(\"label\")\n",
    ")\n",
    "\n",
    "df_ml = df_ml.join(top_labels, on=\"label\", how=\"inner\").limit(SAMPLE_SIZE)\n",
    "\n",
    "# Show unique labels\n",
    "print(f\"Selected {TOP_N} categories:\")\n",
    "df_ml.select(\"label\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303bf87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Approach 1: TF-IDF + Logistic Regression\n",
    "\n",
    "Traditional ML approach using term frequency-inverse document frequency features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca3a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF + Logistic Regression Pipeline\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"labelIndex\", maxIter=20)\n",
    "\n",
    "tfidf_pipeline = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    hashingTF,\n",
    "    idf,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "673cb8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 10:53:47 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/01/15 10:54:03 WARN TaskSetManager: Stage 107 contains a task of very large size (1297 KiB). The maximum recommended task size is 1000 KiB.\n",
      "26/01/15 10:54:06 WARN DAGScheduler: Broadcasting large task binary with size 1660.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "|abstract                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |label   |prediction|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "|  $UBVRI$ photometry and medium resolution optical spectroscopy of peculiar\\nType Ia supernova SN 2005hk are presented and analysed, covering the\\npre-maximum phase to around 400 days after explosion. The supernova is found to\\nbe underluminous compared to \"normal\" Type Ia supernovae. The photometric and\\nspectroscopic evolution of SN 2005hk is remarkably similar to the peculiar Type\\nIa event SN 2002cx. The expansion velocity of the supernova ejecta is found to\\nbe lower than normal Type Ia events. The spectra obtained $\\gsim 200$ days\\nsince explosion do not show the presence of forbidden [\\ion{Fe}{ii}],\\n[\\ion{Fe}{iii}] and [\\ion{Co}{iii}] lines, but are dominated by narrow,\\npermitted \\ion{Fe}{ii}, NIR \\ion{Ca}{ii} and \\ion{Na}{i} lines with P-Cygni\\nprofiles. Thermonuclear explosion model with Chandrasekhar mass ejecta and a\\nkinetic energy smaller ($\\KE = 0.3 \\times 10^{51} {\\rm ergs}$) than that of\\ncanonical Type Ia supernovae is found to well explain the observed bolometric\\nlight curve. The mass of \\Nifs synthesized in this explosion is $0.18 \\Msun$.\\nThe early spectra are successfully modeled with this less energetic model with\\nsome modifications of the abundance distribution. The late spectrum is\\nexplained as a combination of a photospheric component and a nebular component.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |astro-ph|0.0       |\n",
      "|  (ABRIDGED) We present a new Schwarzschild orbit-superposition code designed\\nto model discrete datasets composed of velocities of individual kinematic\\ntracers in a dynamical system. This constitutes an extension of previous\\nimplementations that can only address continuous data in the form of (the\\nmoments of) velocity distributions, thus avoiding potentially important losses\\nof information due to data binning. Furthermore, the code can handle any\\ncombination of available velocity components, i.e., only line-of-sight\\nvelocities, only proper motions, or a combination of both. It can also handle a\\ncombination of discrete and continuous data. The code finds the distribution\\nfunction (DF, a function of the three integrals of motion E, Lz, and I3) that\\nbest reproduces the available kinematic and photometric observations in a given\\naxisymmetric gravitational potential. The fully numerical approach ensures\\nconsiderable freedom on the form of the DF f(E,Lz,I3). This allows a very\\ngeneral modeling of the orbital structure, thus avoiding restrictive\\nassumptions about the degree of (an)isotropy of the orbits. We describe the\\nimplementation of the discrete code and present a series of tests of its\\nperformance based on the modeling of simulated datasets generated from a known\\nDF. We find that the discrete Schwarzschild code recovers the original orbital\\nstructure, M/L ratios, and inclination of the input datasets to satisfactory\\naccuracy, as quantified by various statistics. The code will be valuable, e.g.,\\nfor modeling stellar motions in Galactic globular clusters, and those of\\nindividual stars, planetary nebulae, or globular clusters in nearby galaxies.\\nThis can shed new light on the total mass distributions of these systems, with\\ncentral black holes and dark matter halos being of particular interest.\\n                        |astro-ph|0.0       |\n",
      "|  (ABRIDGED)- The physical mechanism responsible for the short outbursts in a\\nrecently recognized class of High Mass X-ray Binaries, the Supergiant Fast\\nX-ray Transients (SFXTs), is still unknown. Two main hypotheses have been\\nproposed to date: the sudden accretion by the compact object of small ejections\\noriginating in a clumpy wind from the supergiant donor, or outbursts produced\\nat (or near) the periastron passage in wide and eccentric orbits, in order to\\nexplain the low (1E32 erg/s) quiescent emission.Neither proposed mechanisms\\nseem to explain the whole phenomenology of these sources. Here we propose a new\\nexplanation for the outburst mechanism, based on new X-ray observations of the\\nunique SFXT known to display periodic outbursts, IGRJ11215-5952. We performed\\nthree Target of Opportunity observations with Swift, XMM-Newton and INTEGRAL at\\nthe time of the fifth outburst, expected on 2007 February 9. Swift observations\\nof the February 2007 outburst have been reported elsewhere. Another ToO with\\nSwift was performed in July 2007, in order to monitor the supposed ``apastron''\\npassage. A second unexpected outburst was discovered on 2007 July 24, after\\nabout 165 days from the February 2007 outburst. The new X-ray observations\\nallow us to propose an alternative hypothesis for the outburst mechanism in\\nSFXTs, linked to the possible presence of a second wind component,in the form\\nof an equatorial disk from the supergiant donor. We discuss the applicability\\nof the model to the short outburst durations of all other SFXTs, where a clear\\nperiodicity in the outbursts has not been found yet. The new outburst from\\nIGRJ11215-5952 observed in July suggests that the true orbital period is\\n~165days, instead of 329days, as previously thought.\\n                                                                                          |astro-ph|0.0       |\n",
      "|  (Abridged) Among the various issues that remain open in the field of\\naccretion onto black hole X-ray binaries (BHBs) is the way the gas accretes at\\nvery low Eddington ratios, in the so-called quiescent regime. While there is\\ngeneral agreement that the X-rays are produced by a population of high-energy\\nelectrons near to the BH, the controversy comes about in modeling the\\ncontribution from inflowing vs. outflowing particles, and their relative energy\\nbudget. Recent Spitzer observations of three quiescent BHBs have shown evidence\\nfor excess emission with respect to the tail of the companion star between 8-24\\nmicron. We suggest that synchrotron emission from a partially self-absorbed\\noutflow might be responsible for the observed mid-IR excess, in place of, or in\\naddition to, thermal emission from circumbinary material. If so, then the jet\\nsynchrotron luminosity exceeds the measured 2-10 keV luminosity by a factor of\\na few in these systems. In turn, the mechanical power stored in the jet exceeds\\nthe bolometric X-ray luminosity at least by 4 orders of magnitude. We then\\ncompile the broadband spectral energy distribution (SED) of A0620-00, the\\nlowest Eddington-ratio stellar mass BH with a known radio counterpart, by means\\nof simultaneous radio, optical and X-ray observations, and the archival Spitzer\\ndata. We are able to fit the SED of A0620-00 with a `maximally jet-dominated'\\nmodel in which the radio through the soft X-rays are dominated by synchrotron\\nemission, while the hard X-rays are dominated by inverse Compton at the jet\\nbase. The fitted parameters land in a range of values that is reminiscent of\\nthe Galactic Center super-massive BH Sgr A*. Most notably, the inferred ratio\\nof the jet acceleration rate to local cooling rates is two orders of magnitude\\nweaker with respect to higher luminosity, hard state sources.\\n|astro-ph|0.0       |\n",
      "|  (Abridged) Mass-loss from massive stars leads to the formation of\\ncircumstellar wind-blown bubbles surrounding the star, bordered by a dense\\nshell. When the star ends its life in a supernova (SN) explosion, the resulting\\nshock wave will interact with this modified medium. In a previous paper we\\ndiscussed the basic parameters of this interaction. In this paper we go a step\\nfurther and study the evolution of SNe in the wind blown bubble formed by a 35\\n$\\msun$ star that starts off as an O star, goes through a red supergiant phase,\\nand ends its life as a Wolf-Rayet star. We model the evolution of the CSM and\\nthen the expansion of the SN shock wave within this medium. Our simulations\\nclearly reveal fluctuations in density and pressure within the surrounding\\nmedium. The SN shock interacting with these fluctuations, and then with the\\ndense shell surrounding the wind-blown cavity, gives rise to a variety of\\ntransmitted and reflected shocks in the wind bubble. The interactions between\\nthese various shocks and discontinuities is examined, and its effects on the\\nX-ray emission is noted. Our simulations reveal the presence of several\\nhydrodynamic instabilities. They show that the turbulent interior, coupled with\\nthe large fluctuations in density and pressure, gives rise to an extremely\\ncorrugated SN shock wave. The shock shows considerable wrinkles as it impacts\\nthe dense shell, and the impact occurs in a piecemeal fashion, with some parts\\nof the shock wave interacting with the shell before the others. Therefore\\ndifferent parts of the shell will `light-up' at different times. The\\nnon-spherical nature of the interaction means that it will occur over a\\nprolonged period of time, and the spherical symmetry of the initial shock wave\\nis destroyed.\\n                                                                                   |astro-ph|0.0       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Train/test split and model training\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "tfidf_model = tfidf_pipeline.fit(train)\n",
    "tfidf_model.write().overwrite().save(\"models/logreg_text_classifier\")\n",
    "\n",
    "predictions = tfidf_model.transform(test)\n",
    "predictions.select(\"abstract\", \"label\", \"prediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4958a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 10:54:10 WARN DAGScheduler: Broadcasting large task binary with size 1716.2 KiB\n",
      "26/01/15 10:54:14 WARN DAGScheduler: Broadcasting large task binary with size 1716.2 KiB\n",
      "[Stage 128:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + LogReg Results:\n",
      "  Accuracy: 0.7898\n",
      "  F1 Score: 0.7921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Evaluate TF-IDF model\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(f\"TF-IDF + LogReg Results:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9def0521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 10:54:18 WARN DAGScheduler: Broadcasting large task binary with size 1683.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|labelIndex|prediction|count|\n",
      "+----------+----------+-----+\n",
      "|       0.0|       0.0| 1241|\n",
      "|       0.0|       1.0|   45|\n",
      "|       0.0|       2.0|   24|\n",
      "|       0.0|       3.0|   16|\n",
      "|       0.0|       4.0|   60|\n",
      "|       0.0|       5.0|    7|\n",
      "|       0.0|       6.0|   11|\n",
      "|       0.0|       7.0|    9|\n",
      "|       1.0|       0.0|   23|\n",
      "|       1.0|       1.0|  493|\n",
      "|       1.0|       2.0|   49|\n",
      "|       1.0|       3.0|    8|\n",
      "|       1.0|       4.0|    7|\n",
      "|       1.0|       6.0|    5|\n",
      "|       1.0|       7.0|    2|\n",
      "|       2.0|       0.0|    6|\n",
      "|       2.0|       1.0|   41|\n",
      "|       2.0|       2.0|  383|\n",
      "|       2.0|       3.0|   18|\n",
      "|       2.0|       4.0|   51|\n",
      "|       2.0|       5.0|    4|\n",
      "|       2.0|       6.0|    2|\n",
      "|       2.0|       7.0|    2|\n",
      "|       3.0|       0.0|    3|\n",
      "|       3.0|       1.0|    7|\n",
      "|       3.0|       2.0|   20|\n",
      "|       3.0|       3.0|  355|\n",
      "|       3.0|       4.0|    8|\n",
      "|       3.0|       5.0|   12|\n",
      "|       3.0|       6.0|   25|\n",
      "|       3.0|       7.0|    5|\n",
      "|       4.0|       0.0|   28|\n",
      "|       4.0|       1.0|    4|\n",
      "|       4.0|       2.0|   62|\n",
      "|       4.0|       3.0|   12|\n",
      "|       4.0|       4.0|  184|\n",
      "|       4.0|       5.0|    3|\n",
      "|       4.0|       6.0|    1|\n",
      "|       5.0|       0.0|    6|\n",
      "|       5.0|       1.0|    9|\n",
      "|       5.0|       2.0|    8|\n",
      "|       5.0|       3.0|   11|\n",
      "|       5.0|       4.0|    4|\n",
      "|       5.0|       5.0|  141|\n",
      "|       5.0|       6.0|   44|\n",
      "|       5.0|       7.0|   18|\n",
      "|       6.0|       0.0|    3|\n",
      "|       6.0|       1.0|    5|\n",
      "|       6.0|       2.0|    6|\n",
      "|       6.0|       3.0|   19|\n",
      "+----------+----------+-----+\n",
      "only showing top 50 rows\n",
      "\n",
      "\n",
      "Label Index Mapping:\n",
      "  0: astro-ph\n",
      "  1: hep-ph\n",
      "  2: hep-th\n",
      "  3: quant-ph\n",
      "  4: gr-qc\n",
      "  5: cond-mat.mtrl-sci\n",
      "  6: cond-mat.mes-hall\n",
      "  7: cond-mat.str-el\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "confusion = predictions.groupBy(\"labelIndex\", \"prediction\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"labelIndex\", \"prediction\")\n",
    "confusion.show(50)\n",
    "\n",
    "# Label mapping\n",
    "labels = tfidf_model.stages[0].labels\n",
    "print(\"\\nLabel Index Mapping:\")\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"  {i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9090c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 139:=============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSTRACT:\n",
      "  $UBVRI$ photometry and medium resolution optical spectroscopy of peculiar\n",
      "Type Ia supernova SN 2005hk are presented and analysed, covering the\n",
      "pre-maximum phase to around 400 days after explosion. The supernova is found to\n",
      "be underluminous compared to \"normal\" Type Ia supernovae. The photometric and\n",
      "spectroscopic evolution of SN 2005hk is remarkably similar to the peculiar Type\n",
      "Ia event SN 2002cx. The expansion velocity of the supernova ejecta is found to\n",
      "be lower than normal Type Ia events. T ...\n",
      "\n",
      "TRUE CATEGORY: astro-ph\n",
      "PREDICTED CATEGORY: astro-ph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 10:54:22 WARN DAGScheduler: Broadcasting large task binary with size 1659.9 KiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "example = predictions.select(\"abstract\", \"label\", \"prediction\").limit(1).collect()[0]\n",
    "\n",
    "true_label = example[\"label\"]\n",
    "pred_index = int(example[\"prediction\"])\n",
    "pred_label = labels[pred_index]\n",
    "\n",
    "print(\"ABSTRACT:\")\n",
    "print(example[\"abstract\"][:500], \"...\")\n",
    "print(f\"\\nTRUE CATEGORY: {true_label}\")\n",
    "print(f\"PREDICTED CATEGORY: {pred_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4eb88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Approach 2: DistilBERT + ClassifierDL\n",
    "\n",
    "Deep learning approach using pre-trained DistilBERT embeddings with Spark NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "410a60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for DistilBERT (smaller sample due to computational cost)\n",
    "BERT_SAMPLE_SIZE = 200\n",
    "BERT_TOP_N = 20\n",
    "\n",
    "df_bert = df.select(\"abstract\", \"categories\") \\\n",
    "            .withColumn(\"label\", split(col(\"categories\"), \" \").getItem(0)) \\\n",
    "            .select(col(\"abstract\").alias(\"text\"), \"label\") \\\n",
    "            .na.drop()\n",
    "\n",
    "bert_top_labels = (\n",
    "    df_bert.groupBy(\"label\")\n",
    "           .agg(count(\"*\").alias(\"count\"))\n",
    "           .orderBy(col(\"count\").desc())\n",
    "           .limit(BERT_TOP_N)\n",
    "           .select(\"label\")\n",
    ")\n",
    "\n",
    "df_bert = df_bert.join(bert_top_labels, on=\"label\", how=\"inner\").limit(BERT_SAMPLE_SIZE)\n",
    "train_bert, test_bert = df_bert.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b47a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert_base_uncased download started this may take some time.\n",
      "Approximate size to download 235.8 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/15 10:54:32 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "26/01/15 10:54:32 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert_base_uncased download started this may take some time.\n",
      "Approximate size to download 235.8 MB\n",
      "Download done! Loading the resource.\n",
      "[ \\ ]Using CPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/local/spark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Build DistilBERT Pipeline\n",
    "document = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer_bert = SparkNLPTokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "distilbert = DistilBertEmbeddings.pretrained(\"distilbert_base_uncased\", \"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"sentence_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "classifier = ClassifierDLApproach() \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCol(\"prediction\") \\\n",
    "    .setLabelColumn(\"label\") \\\n",
    "    .setBatchSize(8) \\\n",
    "    .setMaxEpochs(3) \\\n",
    "    .setLr(1e-3) \\\n",
    "    .setEnableOutputLogs(True)\n",
    "\n",
    "bert_pipeline = Pipeline(stages=[\n",
    "    document,\n",
    "    tokenizer_bert,\n",
    "    distilbert,\n",
    "    sentence_embeddings,\n",
    "    classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85c92f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 10:55:47.969201: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/ffa7b793c862_classifier_dl15392169626878701214\n",
      "2026-01-15 10:55:48.117838: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2026-01-15 10:55:48.118034: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /tmp/ffa7b793c862_classifier_dl15392169626878701214\n",
      "2026-01-15 10:55:48.119540: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-15 10:55:48.882626: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2026-01-15 10:55:50.169813: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/ffa7b793c862_classifier_dl15392169626878701214\n",
      "2026-01-15 10:55:50.347454: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 2378226 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started - epochs: 3 - learning_rate: 0.001 - batch_size: 8 - training_examples: 169 - classes: 17\n",
      "Epoch 1/3 - 3.28s - loss: 59.748936 - acc: 0.23809524 - batches: 22\n",
      "Epoch 2/3 - 0.11s - loss: 58.44898 - acc: 0.25 - batches: 22\n",
      "Epoch 3/3 - 0.12s - loss: 58.44899 - acc: 0.25 - batches: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 176:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+--------+---------------+\n",
      "|                                                                            text|   label|predicted_label|\n",
      "+--------------------------------------------------------------------------------+--------+---------------+\n",
      "|  Aims and Methods: We present the results of VLBI observations of nineteen\\n...|astro-ph|       astro-ph|\n",
      "|  By combining high-resolution HST and wide-field ground based observations, ...|astro-ph|       astro-ph|\n",
      "|  Common envelopes form in dynamical time scale mass exchange, when the\\nenve...|astro-ph|       astro-ph|\n",
      "|  Gamma-Ray Bursts (GRBs) have been detected at GeV energies by EGRET and\\nmo...|astro-ph|       astro-ph|\n",
      "|  Some of the means through which the possible presence of nearly deconfined\\...|astro-ph|       astro-ph|\n",
      "+--------------------------------------------------------------------------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Train DistilBERT model\n",
    "bert_model = bert_pipeline.fit(train_bert)\n",
    "bert_model.write().overwrite().save(\"models/distilbert_classifier\")\n",
    "\n",
    "bert_predictions = bert_model.transform(test_bert)\n",
    "bert_predictions.select(\n",
    "    col(\"text\"),\n",
    "    col(\"label\"),\n",
    "    col(\"prediction.result\")[0].alias(\"predicted_label\")\n",
    ").show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1700cace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 201:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT + ClassifierDL Results:\n",
      "  Accuracy: 0.4516\n",
      "  F1 Score: 0.2810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Evaluate DistilBERT model\n",
    "# Extract predicted label from Spark NLP annotation\n",
    "bert_predictions_eval = bert_predictions.withColumn(\n",
    "    \"predicted_label\",\n",
    "    expr(\"prediction.result[0]\")\n",
    ")\n",
    "\n",
    "# Convert string labels to numeric indices for evaluation\n",
    "label_indexer_eval = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "pred_indexer_eval = StringIndexer(inputCol=\"predicted_label\", outputCol=\"predicted_index\")\n",
    "\n",
    "# Fit on true labels and transform both columns\n",
    "label_indexer_model = label_indexer_eval.fit(bert_predictions_eval)\n",
    "bert_predictions_indexed = label_indexer_model.transform(bert_predictions_eval)\n",
    "\n",
    "pred_indexer_model = pred_indexer_eval.fit(bert_predictions_indexed)\n",
    "bert_predictions_indexed = pred_indexer_model.transform(bert_predictions_indexed)\n",
    "\n",
    "# Now evaluate with numeric columns\n",
    "bert_evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"predicted_index\", metricName=\"accuracy\"\n",
    ")\n",
    "bert_evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"predicted_index\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "bert_accuracy = bert_evaluator_acc.evaluate(bert_predictions_indexed)\n",
    "bert_f1 = bert_evaluator_f1.evaluate(bert_predictions_indexed)\n",
    "\n",
    "print(f\"DistilBERT + ClassifierDL Results:\")\n",
    "print(f\"  Accuracy: {bert_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {bert_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7df186-cab7-41b6-9cf2-91372ff0258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
