{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c5657f",
   "metadata": {},
   "source": [
    "# SparkX Analysis\n",
    "\n",
    "This notebook performs a full-scale analysis of the ArXiv citation network, using a distributed Apache Spark cluster. \n",
    "\n",
    "Unlike the initial version of this analysis, this workflow is optimized for Big Data: it reads raw data directly from HDFS, processes citations using distributed functions, and persists intermediate results in Parquet format to overcome memory issues. The analysis utilizes GraphFrames to build the graph and execute complex algorithms at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark==3.5.0\n",
    "# # you need to use Python 3.10 or 3.11 or else it does not work\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install tqdm\n",
    "# !pip install graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87f727",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84272aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pyspark configuration\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col, explode, split, size, array, lit\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# for graphx we use graphframes\n",
    "from graphframes import GraphFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40f5fb",
   "metadata": {},
   "source": [
    "### Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session with optimized configuration\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Previous session closed\")\n",
    "except:\n",
    "    print(\"No previous session\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analisis Arxiv - Driver Potente\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark session created: {spark.version}\")\n",
    "print(f\"Spark context initiated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92423b8d",
   "metadata": {},
   "source": [
    "### Citation extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8da235",
   "metadata": {},
   "source": [
    "In this cell we define the main functions to transform the raw json data into a network structure. We define 2 functions: `extract_arxiv_ids_from_text`, an auxiliar function that uses patterns (regex) to find ArXiv ids hidden within texts like abstracts, and `build_citation_graph`, which reads the file directly using Spark Dataframes, extracts the paper details to create the **nodes** and scans the text for reference to create the **edges** (citations), saving the final results directly to HDFS to ensure scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "687c1260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==============================================>         (10 + 1) / 12]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField\n",
    "from pyspark.sql.functions import udf, col, explode, struct\n",
    "\n",
    "def extract_arxiv_ids_from_text(text):\n",
    "    \"\"\"\n",
    "    extracts arxiv ids from text using regex\n",
    "    common formats: arXiv:1234.5678, arXiv:1234.5678v1, 1234.5678\n",
    "    \"\"\"\n",
    "    if not text or text is None:\n",
    "        return []\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # pattern to detect arxiv ids\n",
    "    patterns = [\n",
    "        r'arXiv:(\\d{4}\\.\\d{4,5})(v\\d+)?',  # arXiv:1234.5678v1\n",
    "        r'arxiv:(\\d{4}\\.\\d{4,5})(v\\d+)?',   # arxiv:1234.5678\n",
    "        r'(?<![.\\d])(\\d{4}\\.\\d{4,5})(?![.\\d])',  # 1234.5678 \n",
    "    ]\n",
    "    \n",
    "    ids = set()\n",
    "    for pattern in patterns:\n",
    "        matches = re.finditer(pattern, str(text), re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            arxiv_id = match.group(1)\n",
    "            ids.add(arxiv_id)\n",
    "    \n",
    "    return list(ids)\n",
    "\n",
    "\n",
    "def build_citation_graph(spark, data_path, output_path, sample_size=None):\n",
    "    \"\"\"\n",
    "    builds the citation graph from the arxiv json file using distributed processing.\n",
    "    saves results directly to HDFS without collecting to driver.\n",
    "    \n",
    "    arguments:\n",
    "        spark: active spark session\n",
    "        data_path: path to the arxiv metadata json file (HDFS)\n",
    "        output_path: base path to save results in HDFS\n",
    "        sample_size: if specified only processes the first N papers\n",
    "    \n",
    "    returns:\n",
    "        nodes_count: number of nodes processed\n",
    "        edges_count: number of edges found\n",
    "    \"\"\"\n",
    "    print(f\"\\nReading papers from {data_path}...\")\n",
    "    \n",
    "    # read JSON directly with Spark (distributed)\n",
    "    df = spark.read.json(data_path)\n",
    "    \n",
    "    if sample_size:\n",
    "        df = df.limit(sample_size)\n",
    "    \n",
    "    total_papers = df.count()\n",
    "    print(f\"Papers to process: {total_papers:,}\")\n",
    "    \n",
    "    # extract node information (distributed operation)\n",
    "    print(\"\\nExtracting node information...\")\n",
    "    nodes_spark = df.select(\n",
    "        col(\"id\"),\n",
    "        col(\"title\"),\n",
    "        col(\"categories\"),\n",
    "        col(\"authors\"),\n",
    "        col(\"abstract\"),\n",
    "        F.when(\n",
    "            F.size(col(\"versions\")) > 0,\n",
    "            F.substring(col(\"versions\").getItem(0).getField(\"created\"), 1, 4)\n",
    "        ).otherwise(\"unknown\").alias(\"year\")\n",
    "    )\n",
    "    \n",
    "    # register UDF for citation extraction (runs in parallel on executors)\n",
    "    extract_citations_udf = udf(extract_arxiv_ids_from_text, ArrayType(StringType()))\n",
    "    \n",
    "    # extract citations \n",
    "    print(\"Extracting citations (distributed)...\")\n",
    "    df_with_citations = df.select(\n",
    "        col(\"id\").alias(\"src\"),\n",
    "        col(\"abstract\"),\n",
    "        col(\"comments\")\n",
    "    ).withColumn(\n",
    "        \"combined_text\",\n",
    "        F.concat_ws(\" \", F.coalesce(col(\"abstract\"), F.lit(\"\")), F.coalesce(col(\"comments\"), F.lit(\"\")))\n",
    "    ).withColumn(\n",
    "        \"cited_ids\",\n",
    "        extract_citations_udf(col(\"combined_text\"))\n",
    "    )\n",
    "    \n",
    "    # create edges by exploding cited_ids\n",
    "    print(\"Creating edges (distributed)...\")\n",
    "    edges_spark = df_with_citations.select(\n",
    "        col(\"src\"),\n",
    "        explode(col(\"cited_ids\")).alias(\"dst\")\n",
    "    ).filter(\n",
    "        col(\"src\") != col(\"dst\")  # avoid self-citations\n",
    "    ).withColumn(\"citing_paper\", col(\"src\")) \\\n",
    "     .withColumn(\"cited_paper\", col(\"dst\")) \\\n",
    "     .distinct()\n",
    "    \n",
    "    # cache for counting\n",
    "    edges_spark.cache()\n",
    "    nodes_spark.cache()\n",
    "    \n",
    "    # count results\n",
    "    num_edges = edges_spark.count()\n",
    "    num_nodes = nodes_spark.count()\n",
    "    \n",
    "    print(f\"\\nPapers processed: {num_nodes:,}\")\n",
    "    print(f\"Citations found: {num_edges:,}\")\n",
    "    \n",
    "    # save to hdfs to avoid issues with the cluster memory\n",
    "    print(f\"\\nSaving results to HDFS at {output_path}...\")\n",
    "    \n",
    "    nodes_spark.write.mode(\"overwrite\").parquet(f\"{output_path}/nodes\")\n",
    "    print(f\"  - Nodes saved to {output_path}/nodes\")\n",
    "    \n",
    "    edges_spark.write.mode(\"overwrite\").parquet(f\"{output_path}/edges\")\n",
    "    print(f\"  - Edges saved to {output_path}/edges\")\n",
    "    \n",
    "    print(\"\\nResults saved successfully in HDFS\")\n",
    "    \n",
    "    return num_nodes, num_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1d093",
   "metadata": {},
   "source": [
    "In this cell we execute the data extraction on the full dataset. It calls the previously defined functions to read the json metadata, identify citation links within the text, and save the results to HDFS.\n",
    "\n",
    "Also, it prints basic statistics to provide an overview of the network's volume and connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ff91f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: REFERENCE AND CITATION EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "Reading papers from hdfs:///data/arxiv-metadata-oai-snapshot.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers to process: 973,085\n",
      "\n",
      "Extracting node information...\n",
      "Extracting citations (distributed)...\n",
      "Creating edges (distributed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 16:45:14 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Papers processed: 973,085\n",
      "Citations found: 40,358\n",
      "\n",
      "Saving results to HDFS at hdfs:///output/citation_graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Nodes saved to hdfs:///output/citation_graph/nodes\n",
      "  - Edges saved to hdfs:///output/citation_graph/edges\n",
      "\n",
      "Results saved successfully in HDFS\n",
      "\n",
      "Basic statistics:\n",
      "  - Nodes (articles): 973,085\n",
      "  - Edges (citations): 40,358\n",
      "  - Average density: 0.04 citations/article\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==============================================>         (10 + 1) / 12]"
     ]
    }
   ],
   "source": [
    "# we need to upload the json document to hdfs before in order to excute this cell\n",
    "# in order to upload the document:\n",
    "# in your cluster terminal:\n",
    "# 1. hdfs dfs -mkdir -p /data # create hdfs directory\n",
    "# 2. hdfs dfs -put arxiv-metadata-oai-snapshot.json /data/ # upload the document (from wherever you have it)\n",
    "# 3. hdfs dfs -ls -h /data/ # to verify it uploaded correctly\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: REFERENCE AND CITATION EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "SAMPLE_SIZE = None  # process all dataset\n",
    "\n",
    "num_nodes, num_edges = build_citation_graph(\n",
    "    spark, \n",
    "    'hdfs:///data/arxiv-metadata-oai-snapshot.json',\n",
    "    'hdfs:///output/citation_graph',  # output path in HDFS\n",
    "    SAMPLE_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"  - Nodes (articles): {num_nodes:,}\")\n",
    "print(f\"  - Edges (citations): {num_edges:,}\")\n",
    "print(f\"  - Average density: {num_edges/num_nodes:.2f} citations/article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057512b",
   "metadata": {},
   "source": [
    "It shows a very low connectivity density (0.04 citations per article). This low connectivity is expected, as the extraction is limited to the abstracts and not full bibliographies. Consequently, the resulting graph will probably consist mostly of isolated nodes and small clusters rather than a dense network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71851b9",
   "metadata": {},
   "source": [
    "### Graph cleaning and filering\n",
    "\n",
    "In this section, we clean the data to keep only valid connections. We filter out citations that point to papers missing from our sample dataset and remove the papers that do not have any links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70c7b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: GRAPH CLEANING AND FILTERING\n",
      "======================================================================\n",
      "\n",
      "Loading data from HDFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 973,085 nodes and 40,358 edges\n",
      "\n",
      "Filtering edges where both papers exist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edges after filtering: 39,867\n",
      "Reduction: 1.2%\n",
      "\n",
      "Identifying connected papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connected nodes: 60,068\n",
      "Main component coverage: 6.2% of total\n",
      "\n",
      "Saving filtered data to HDFS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=============>  (10 + 1) / 12][Stage 95:>                 (0 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to HDFS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==============================================>         (10 + 1) / 12]"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: GRAPH CLEANING AND FILTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# load from hdfs\n",
    "print(\"\\nLoading data from HDFS...\")\n",
    "nodes_spark = spark.read.parquet(\"hdfs:///output/citation_graph/nodes\")\n",
    "edges_spark = spark.read.parquet(\"hdfs:///output/citation_graph/edges\")\n",
    "\n",
    "initial_nodes = nodes_spark.count()\n",
    "initial_edges = edges_spark.count()\n",
    "\n",
    "print(f\"Loaded {initial_nodes:,} nodes and {initial_edges:,} edges\")\n",
    "\n",
    "# filter edges where both papers exist in our dataset \n",
    "print(\"\\nFiltering edges where both papers exist...\")\n",
    "valid_ids = nodes_spark.select(\"id\")\n",
    "\n",
    "edges_filtered = edges_spark.join(\n",
    "    valid_ids.withColumnRenamed(\"id\", \"src_check\"),\n",
    "    edges_spark.src == col(\"src_check\"),\n",
    "    \"inner\"\n",
    ").drop(\"src_check\").join(\n",
    "    valid_ids.withColumnRenamed(\"id\", \"dst_check\"),\n",
    "    edges_spark.dst == col(\"dst_check\"),\n",
    "    \"inner\"\n",
    ").drop(\"dst_check\")\n",
    "\n",
    "num_edges_filtered = edges_filtered.count()\n",
    "\n",
    "print(f\"\\nEdges after filtering: {num_edges_filtered:,}\")\n",
    "print(f\"Reduction: {(1 - num_edges_filtered/initial_edges)*100:.1f}%\")\n",
    "\n",
    "# identify papers with at least one citation \n",
    "print(\"\\nIdentifying connected papers...\")\n",
    "connected_src = edges_filtered.select(col(\"src\").alias(\"id\"))\n",
    "connected_dst = edges_filtered.select(col(\"dst\").alias(\"id\"))\n",
    "connected_ids = connected_src.union(connected_dst).distinct()\n",
    "\n",
    "nodes_filtered = nodes_spark.join(connected_ids, \"id\", \"inner\")\n",
    "\n",
    "num_nodes_filtered = nodes_filtered.count()\n",
    "\n",
    "print(f\"\\nConnected nodes: {num_nodes_filtered:,}\")\n",
    "print(f\"Main component coverage: {num_nodes_filtered/initial_nodes*100:.1f}% of total\")\n",
    "\n",
    "# cache filtered results for next steps\n",
    "edges_filtered.cache()\n",
    "nodes_filtered.cache()\n",
    "\n",
    "# save processed data to HDFS\n",
    "print(\"\\nSaving filtered data to HDFS...\")\n",
    "nodes_filtered.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/nodes_filtered\")\n",
    "edges_filtered.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/edges_filtered\")\n",
    "\n",
    "print(\"Filtered data saved to HDFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb564918",
   "metadata": {},
   "source": [
    "The filtering process has removed 1.2% of the edges, leaving 39,867 valid citations. More importantly, the active subgraph consists of 60,068 papers (approx. 6.2% of the total dataset), which confirms that the vast majority of articles are isolated and do not form part of the citation network. For the following steps of this analysis, we will focus only on this active sub-group of articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e2147",
   "metadata": {},
   "source": [
    "### Graph construction in Spark\n",
    "\n",
    "In this step, we load the filtered citation data directly from HDFS where it was previously stored in distributed Parquet format. The vertices (papers) and edges (citations) are already stored as Spark DataFrames across the cluster, we combine these distributed DataFrames to create a `GraphFrame` object. This object is the specific format required to run the parallel graph algorithms in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: GRAPH CONSTRUCTION IN SPARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# load filtered data from hdfs (already spark dataframes)\n",
    "print(\"\\nLoading filtered data from HDFS...\")\n",
    "vertices = spark.read.parquet(\"hdfs:///output/citation_graph/nodes_filtered\")\n",
    "edges = spark.read.parquet(\"hdfs:///output/citation_graph/edges_filtered\")\n",
    "\n",
    "# create graphframe (interface for graphx in python)\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "print(f\"\\nGraph created in Spark\")\n",
    "print(f\"  - Vertices: {graph.vertices.count():,}\")\n",
    "print(f\"  - Edges: {graph.edges.count():,}\")\n",
    "\n",
    "# basic graph information\n",
    "print(\"\\nVertex sample:\")\n",
    "graph.vertices.select('id', 'title', 'categories').show(5, truncate=50)\n",
    "\n",
    "print(\"\\nEdge sample:\")\n",
    "graph.edges.select('src', 'dst').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6bde4",
   "metadata": {},
   "source": [
    "### Basic metrics (degree)\n",
    "\n",
    "In this section, we use Spark's distributed processing to calculate two fundamental graph metrics: **In-Degree** and **Out-Degree**. In-Degree measures popularity by counting how many times a paper is cited by others, while Out-Degree measures activity by counting how many references a paper contains. We order these results to identify the top 10 of these 2 metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: BASIC METRICS CALCULATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# in-degree: how many times a paper is cited\n",
    "print(\"\\nCalculating in-degree (most cited papers)...\")\n",
    "in_degrees = graph.inDegrees\n",
    "top_cited = in_degrees.orderBy(col(\"inDegree\").desc())\n",
    "\n",
    "# save to hdfs\n",
    "in_degrees.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/in_degrees\")\n",
    "print(\"In-degrees saved to HDFS\")\n",
    "\n",
    "print(\"\\nTop 10 most cited papers:\")\n",
    "top_cited_sample = top_cited.limit(10)\n",
    "\n",
    "# join with vertex information for display\n",
    "top_cited_with_info = top_cited_sample.join(\n",
    "    graph.vertices.select(\"id\", \"title\", \"categories\"),\n",
    "    \"id\"\n",
    ")\n",
    "\n",
    "# collect only top 10 for display\n",
    "for idx, row in enumerate(top_cited_with_info.collect(), 1):\n",
    "    print(f\"{idx}. [{row['id']}] {row['title'][:60]}...\")\n",
    "    print(f\"   Citations: {row['inDegree']}, Category: {row['categories']}\")\n",
    "\n",
    "# out-degree: how many references a paper makes\n",
    "print(\"\\nCalculating out-degree (papers citing the most)...\")\n",
    "out_degrees = graph.outDegrees\n",
    "top_citing = out_degrees.orderBy(col(\"outDegree\").desc())\n",
    "\n",
    "# save to hdfs\n",
    "out_degrees.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/out_degrees\")\n",
    "print(\"Out-degrees saved to HDFS\")\n",
    "\n",
    "print(\"\\nTop 10 papers making the most references:\")\n",
    "top_citing_sample = top_citing.limit(10)\n",
    "\n",
    "# join with vertex information for display\n",
    "top_citing_with_info = top_citing_sample.join(\n",
    "    graph.vertices.select(\"id\", \"title\", \"categories\"),\n",
    "    \"id\"\n",
    ")\n",
    "\n",
    "# collect only top 10 for display\n",
    "for idx, row in enumerate(top_citing_with_info.collect(), 1):\n",
    "    print(f\"{idx}. [{row['id']}] {row['title'][:60]}...\")\n",
    "    print(f\"   References: {row['outDegree']}, Category: {row['categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df233356",
   "metadata": {},
   "source": [
    "These results reinforce the observation of a high sparsity in the dataset. The most cited article only has 9 citations, and the top citing one has also 9. This confirms that we are observing small and isolated loops and not a complete citation history. \n",
    "\n",
    "Both lists are dominated by Statistics (stat) and Mathematics (math) papers, which suggests that authors in these fields may be more likely to reference other works in their abstracts compared to other disciplines. The presence of the same paper (`0804.0079`) at the top of both lists indicates a tight, self-contained cluster of discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789b9af",
   "metadata": {},
   "source": [
    "### PageRank algorithm\n",
    "\n",
    "In this cell, we apply the **PageRank** algorithm to measure the importance of each paper within the network. Unlike simple citation counts, PageRank determines influence recursively: a paper becomes important if it is cited by other important papers. We execute the algorithm using Spark's distributed GraphX system, extract the top 20 most influential papers, and save the results to HDFS in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6443a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: PAGERANK - MOST INFLUENTIAL PAPERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nRunning PageRank...\")\n",
    "pagerank_results = graph.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "# get top papers by pagerank\n",
    "top_pagerank = pagerank_results.vertices.select(\"id\", \"pagerank\") \\\n",
    "    .orderBy(col(\"pagerank\").desc())\n",
    "\n",
    "# save complete PageRank results to HDFS\n",
    "pagerank_results.vertices.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/pagerank_results\")\n",
    "print(\"PageRank results saved to HDFS\")\n",
    "\n",
    "print(\"\\nTop 20 most influential papers by PageRank:\")\n",
    "top_pr_sample = top_pagerank.limit(20)\n",
    "\n",
    "# join with vertex information for display\n",
    "top_pr_with_info = top_pr_sample.join(\n",
    "    graph.vertices.select(\"id\", \"title\", \"categories\", \"year\"),\n",
    "    \"id\"\n",
    ")\n",
    "\n",
    "# collect only top 20 for display\n",
    "for idx, row in enumerate(top_pr_with_info.collect(), 1):\n",
    "    print(f\"\\n{idx}. PageRank: {row['pagerank']:.4f}\")\n",
    "    print(f\"   ID: {row['id']}\")\n",
    "    print(f\"   Title: {row['title'][:70]}...\")\n",
    "    print(f\"   Categories: {row['categories']}\")\n",
    "    print(f\"   Year: {row['year']}\")\n",
    "\n",
    "# save top results with metadata to HDFS\n",
    "top_pagerank_full = top_pagerank.join(\n",
    "    graph.vertices.select(\"id\", \"title\", \"categories\", \"year\"),\n",
    "    \"id\"\n",
    ")\n",
    "top_pagerank_full.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/top_pagerank_with_metadata\")\n",
    "print(\"\\nTop PageRank papers with metadata saved to HDFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e58eb",
   "metadata": {},
   "source": [
    "The top paper (`0804.0079`) is the same as the most cited paper found in the previous step, confirming it as the central node of this subgraph. \n",
    "\n",
    "The articles in places 2, 3, and 4 share the exact same PageRank score (10.7929). This  typically indicates a symmetric structure (such as a closed loop where the papers cite each other) or a scenario where they are all cited by the same set of external sources. \n",
    "\n",
    "The dominance of Statistics categories confirms that the most active \"islands\" in this sample are concentrated in that field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e18ae0",
   "metadata": {},
   "source": [
    "### Connected components\n",
    "\n",
    "In this step, we run the **connected components** algorithm to decompose the network into independent clusters where every article is linked to the others. The objective is to identify if there is a large central community (\"Giant Component\") connecting most of the researchers or if the graph is fragmented into many isolated groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: CONNECTED COMPONENTS - COMMUNITY DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nCalculating connected components...\")\n",
    "\n",
    "# configure checkpoint directory in HDFS\n",
    "sc.setCheckpointDir(\"hdfs:///tmp/spark_checkpoint\")\n",
    "print(f\"Checkpoint directory: hdfs:///tmp/spark_checkpoint\")\n",
    "\n",
    "try:\n",
    "    print(\"Attempting graphx algorithm...\")\n",
    "    components = graph.connectedComponents(\n",
    "        algorithm=\"graphx\",\n",
    "        checkpointInterval=5\n",
    "    )\n",
    "    \n",
    "    # force execution\n",
    "    component_count = components.select(\"component\").distinct().count()\n",
    "    print(f\"Connected components calculated: {component_count} components found\")\n",
    "    \n",
    "    # save components to HDFS\n",
    "    components.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/connected_components\")\n",
    "    print(\"Connected components saved to HDFS\")\n",
    "    \n",
    "    # analyze component sizes\n",
    "    component_sizes = components.groupBy(\"component\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    \n",
    "    # get total nodes count\n",
    "    total_nodes = components.count()\n",
    "    \n",
    "    # collect only top 10 for display\n",
    "    comp_sizes_top = component_sizes.limit(10).collect()\n",
    "    \n",
    "    print(\"\\nComponent distribution:\")\n",
    "    print(f\"  - Total components: {component_count}\")\n",
    "    print(f\"  - Largest component: {comp_sizes_top[0]['count']:,} papers\")\n",
    "    print(f\"  - % in main component: {comp_sizes_top[0]['count']/total_nodes*100:.1f}%\")\n",
    "\n",
    "    print(\"\\nTop 10 largest components:\")\n",
    "    for row in comp_sizes_top:\n",
    "        print(f\"{row['component']:20} {row['count']:10,}\")\n",
    "\n",
    "    # analyze papers in the main component\n",
    "    main_component_id = comp_sizes_top[0]['component']\n",
    "    main_component_papers = components.filter(col(\"component\") == main_component_id)\n",
    "\n",
    "    print(f\"\\nAnalyzing main component ({main_component_id})...\")\n",
    "    main_comp_categories = main_component_papers.groupBy(\"categories\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc())\n",
    "\n",
    "    print(\"\\nMost common categories in main component:\")\n",
    "    for row in main_comp_categories.limit(15).collect():\n",
    "        print(f\"{row['categories']:40} {row['count']:10,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError calculating connected components: {e}\")\n",
    "    print(\"This is common with small or highly disconnected graphs\")\n",
    "    print(\"Continuing with rest of analysis...\")\n",
    "    components = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44b0b1",
   "metadata": {},
   "source": [
    "The results show extreme fragmentation. We have found 1,979 distinct components, with the largest one containing only 13 articles. This proves there is no central community, and the network is made up of many small and disconnected communities. The largest is a group focused on Representation Theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaeb6a",
   "metadata": {},
   "source": [
    "### Label propagation (LBA)\n",
    "\n",
    "In this step, we apply the **label propagation algorithm** (LPA) to detect communities based on the density of the connections. Unlike connected components, which simply finds linked islands, LPA allows us to identify tight-knit clusters where papers reference each other frequently. \n",
    "Later, we analyze the categories of the largest communities to check if these mathematical clusters correspond to their actual topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704687a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: LABEL PROPAGATION - SUB-COMMUNITY DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nRunning Label Propagation Algorithm...\")\n",
    "lpa_result = graph.labelPropagation(maxIter=5)\n",
    "\n",
    "# save communities to HDFS\n",
    "lpa_result.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/label_propagation\")\n",
    "print(\"Label propagation results saved to HDFS\")\n",
    "\n",
    "# analyze detected communities\n",
    "community_sizes = lpa_result.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "# get total community count\n",
    "total_communities = community_sizes.count()\n",
    "\n",
    "# collect top 15 for display\n",
    "comm_sizes_top = community_sizes.limit(15).collect()\n",
    "\n",
    "print(\"\\nDetected communities:\")\n",
    "print(f\"  - Total communities: {total_communities}\")\n",
    "print(f\"  - Largest community: {comm_sizes_top[0]['count']:,} papers\")\n",
    "\n",
    "print(\"\\nTop 15 largest communities:\")\n",
    "for row in comm_sizes_top:\n",
    "    print(f\"{row['label']:20} {row['count']:10,}\")\n",
    "\n",
    "# analyze categories by community\n",
    "print(\"\\nAnalyzing composition of the 5 largest communities...\")\n",
    "for i in range(min(5, len(comm_sizes_top))):\n",
    "    comm_id = comm_sizes_top[i]['label']\n",
    "    comm_size = comm_sizes_top[i]['count']\n",
    "    \n",
    "    print(f\"\\n--- Community {i+1} (ID: {comm_id}, Size: {comm_size}) ---\")\n",
    "    \n",
    "    comm_papers = lpa_result.filter(col(\"label\") == comm_id)\n",
    "    comm_cats = comm_papers.groupBy(\"categories\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    \n",
    "    print(\"Main categories:\")\n",
    "    for row in comm_cats.limit(5).collect():\n",
    "        print(f\"  - {row['categories']}: {row['count']} papers ({row['count']/comm_size*100:.1f}%)\")\n",
    "\n",
    "# save community results with metadata to HDFS\n",
    "lpa_with_metadata = lpa_result.join(\n",
    "    graph.vertices.select(\"id\", \"title\", \"categories\"),\n",
    "    \"id\"\n",
    ")\n",
    "lpa_with_metadata.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/communities_with_metadata\")\n",
    "print(\"\\nCommunities with metadata saved to HDFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ca1dc",
   "metadata": {},
   "source": [
    "The algorithm identified 3,926 distinct communities, a number very closde to the total number of nodes (4,643), confirming that the network is not only disconneted but also lacks internal density. The largest community found only contains 9 papers. \n",
    "\n",
    "However, the results confirm that the algorithm works well. For example, Community number 5 only contains papers about Finance and Statistics. This proves that the algorithm successfully grouped papers that actually discuss the same topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b133b",
   "metadata": {},
   "source": [
    "### Triangle count and clustering\n",
    "\n",
    "In this step, we count the number of triangles in the graph. A triangle is formed when three papers are all connected to each other (meaning, A cites B, B cites C, and C cites A). This metric helps us identify groups of articles that are strongly connected to each other, and not just linked in a simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 8: TRIANGLE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nCounting triangles in the graph...\")\n",
    "try:\n",
    "    triangle_counts = graph.triangleCount()\n",
    "    \n",
    "    # save triangle counts to HDFS\n",
    "    triangle_counts.write.mode(\"overwrite\").parquet(\"hdfs:///output/citation_graph/triangle_counts\")\n",
    "    print(\"Triangle counts saved to HDFS\")\n",
    "    \n",
    "    # papers with most triangles\n",
    "    top_triangles = triangle_counts.orderBy(col(\"count\").desc())\n",
    "    \n",
    "    print(\"\\nTop 10 papers with most triangles:\")\n",
    "    top_tri_sample = top_triangles.limit(10)\n",
    "    \n",
    "    # join with vertex information for display\n",
    "    top_tri_with_info = top_tri_sample.join(\n",
    "        graph.vertices.select(\"id\", \"title\"),\n",
    "        \"id\"\n",
    "    )\n",
    "    \n",
    "    # collect only top 10 for display\n",
    "    for idx, row in enumerate(top_tri_with_info.collect(), 1):\n",
    "        print(f\"{idx}. Triangles: {row['count']}\")\n",
    "        print(f\"   ID: {row['id']}\")\n",
    "        print(f\"   Title: {row['title'][:60]}...\")\n",
    "    \n",
    "    # calculate total triangles\n",
    "    total_triangles = triangle_counts.select(F.sum(\"count\")).collect()[0][0]\n",
    "    print(f\"\\nTotal triangles: {total_triangles:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error calculating triangles: {e}\")\n",
    "    print(\"   (This may fail on very large graphs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d0a93",
   "metadata": {},
   "source": [
    "These results show that the articles with the most triangles are part of related discassions. We can observe that in the titles, which include words like \"Reply\", \"Response\" or \"Comment\". This indicates that the triangles in this graph are formed by authors citing each other back and forth discussing specific results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41560b",
   "metadata": {},
   "source": [
    "### Visualizations and statistical analysis\n",
    "\n",
    "This cell genarates six visualizations to summarize the graph's structure. It uses histograms to visualize how citations are distributed, bar charts to show the size of the communities and top papers, and a scatter plot to show the correlation betwwn popularity (count of citations) and influence (PageRank).\n",
    "\n",
    "**ADAPT IN CASE WE NEED TO DO SAMPLING FOR THE VISUALIZATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6995323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1: without sampling, visualizing the whole dataset\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 9: VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# load data from HDFS for visualization\n",
    "print(\"\\nLoading data from HDFS for visualization...\")\n",
    "in_degrees_viz = spark.read.parquet(\"hdfs:///output/citation_graph/in_degrees\").toPandas()\n",
    "out_degrees_viz = spark.read.parquet(\"hdfs:///output/citation_graph/out_degrees\").toPandas()\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "\n",
    "# get top 15 pagerank\n",
    "top_pr_viz = spark.read.parquet(\"hdfs:///output/citation_graph/top_pagerank_with_metadata\") \\\n",
    "    .limit(15).toPandas()\n",
    "\n",
    "# get top 20 component sizes\n",
    "comp_sizes_viz = spark.read.parquet(\"hdfs:///output/citation_graph/connected_components\") \\\n",
    "    .groupBy(\"component\").count().orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20).toPandas()\n",
    "\n",
    "# get top 20 community sizes\n",
    "comm_sizes_viz = spark.read.parquet(\"hdfs:///output/citation_graph/label_propagation\") \\\n",
    "    .groupBy(\"label\").count().orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20).toPandas()\n",
    "\n",
    "# pagerank vs in-degree (join and collect all)\n",
    "pr_indeg_viz = spark.read.parquet(\"hdfs:///output/citation_graph/pagerank_results\") \\\n",
    "    .join(in_degrees_viz, \"id\") \\\n",
    "    .toPandas()\n",
    "\n",
    "# configure matplotlib\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# in-degree distribution\n",
    "axes[0, 0].hist(in_degrees_viz['inDegree'], bins=50, edgecolor='black', color='steelblue')\n",
    "axes[0, 0].set_xlabel('In-Degree (citations received)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('In-Degree Distribution')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# out-degree distribution\n",
    "axes[0, 1].hist(out_degrees_viz['outDegree'], bins=50, edgecolor='black', color='coral')\n",
    "axes[0, 1].set_xlabel('Out-Degree (references made)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Out-Degree Distribution')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# top 15 papers by pagerank\n",
    "axes[0, 2].barh(range(len(top_pr_viz)), top_pr_viz['pagerank'], color='darkgreen')\n",
    "axes[0, 2].set_yticks(range(len(top_pr_viz)))\n",
    "axes[0, 2].set_yticklabels([f\"{row['id'][:10]}...\" for _, row in top_pr_viz.iterrows()])\n",
    "axes[0, 2].set_xlabel('PageRank Score')\n",
    "axes[0, 2].set_title('Top 15 Papers by PageRank')\n",
    "axes[0, 2].invert_yaxis()\n",
    "\n",
    "# component sizes\n",
    "axes[1, 0].bar(range(len(comp_sizes_viz)), \n",
    "               comp_sizes_viz['count'], \n",
    "               color='mediumpurple', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Component ID')\n",
    "axes[1, 0].set_ylabel('Size')\n",
    "axes[1, 0].set_title('Size of Top 20 Connected Components')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# community sizes (lpa)\n",
    "axes[1, 1].bar(range(len(comm_sizes_viz)), \n",
    "               comm_sizes_viz['count'], \n",
    "               color='orange', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Community ID')\n",
    "axes[1, 1].set_ylabel('Size')\n",
    "axes[1, 1].set_title('Size of Top 20 Communities')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "# pagerank vs in-degree\n",
    "axes[1, 2].scatter(pr_indeg_viz['inDegree'], pr_indeg_viz['pagerank'], \n",
    "                   alpha=0.5, s=20, color='crimson')\n",
    "axes[1, 2].set_xlabel('In-Degree')\n",
    "axes[1, 2].set_ylabel('PageRank')\n",
    "axes[1, 2].set_title('PageRank vs Citations Correlation')\n",
    "axes[1, 2].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('arxiv_graph_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualizations saved to 'arxiv_graph_analysis.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2: sampling, taking into account that the number of documents is very big, so it may fail\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 9: VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# load data from HDFS for visualization\n",
    "print(\"\\nLoading data from HDFS for visualization...\")\n",
    "in_degrees_viz = spark.read.parquet(\"hdfs:///output/citation_graph/in_degrees\")\n",
    "out_degrees_viz = spark.read.parquet(\"hdfs:///output/citation_graph/out_degrees\")\n",
    "\n",
    "# sample data for visualization (avoid collecting everything)\n",
    "print(\"Sampling data for efficient visualization...\")\n",
    "in_deg_sample = in_degrees_viz.sample(fraction=0.5, seed=42).toPandas()\n",
    "out_deg_sample = out_degrees_viz.sample(fraction=0.5, seed=42).toPandas()\n",
    "\n",
    "# get top 15 pagerank\n",
    "top_pr_viz = spark.read.parquet(\"hdfs:///output/citation_graph/top_pagerank_with_metadata\") \\\n",
    "    .limit(15).toPandas()\n",
    "\n",
    "# get top 20 component sizes\n",
    "comp_sizes_viz = spark.read.parquet(\"hdfs:///output/citation_graph/connected_components\") \\\n",
    "    .groupBy(\"component\").count().orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20).toPandas()\n",
    "\n",
    "# get top 20 community sizes\n",
    "comm_sizes_viz = spark.read.parquet(\"hdfs:///output/citation_graph/label_propagation\") \\\n",
    "    .groupBy(\"label\").count().orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20).toPandas()\n",
    "\n",
    "# pagerank vs in-degree (sample)\n",
    "pr_indeg_viz = spark.read.parquet(\"hdfs:///output/citation_graph/pagerank_results\") \\\n",
    "    .join(in_degrees_viz, \"id\") \\\n",
    "    .sample(fraction=0.2, seed=42) \\\n",
    "    .toPandas()\n",
    "\n",
    "print(\"Data loaded and sampled successfully\")\n",
    "\n",
    "# configure matplotlib\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# in-degree distribution\n",
    "axes[0, 0].hist(in_deg_sample['inDegree'], bins=50, edgecolor='black', color='steelblue')\n",
    "axes[0, 0].set_xlabel('In-Degree (citations received)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('In-Degree Distribution')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# out-degree distribution\n",
    "axes[0, 1].hist(out_deg_sample['outDegree'], bins=50, edgecolor='black', color='coral')\n",
    "axes[0, 1].set_xlabel('Out-Degree (references made)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Out-Degree Distribution')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# top 15 papers by pagerank\n",
    "axes[0, 2].barh(range(len(top_pr_viz)), top_pr_viz['pagerank'], color='darkgreen')\n",
    "axes[0, 2].set_yticks(range(len(top_pr_viz)))\n",
    "axes[0, 2].set_yticklabels([f\"{row['id'][:10]}...\" for _, row in top_pr_viz.iterrows()])\n",
    "axes[0, 2].set_xlabel('PageRank Score')\n",
    "axes[0, 2].set_title('Top 15 Papers by PageRank')\n",
    "axes[0, 2].invert_yaxis()\n",
    "\n",
    "# component sizes\n",
    "axes[1, 0].bar(range(len(comp_sizes_viz)), \n",
    "               comp_sizes_viz['count'], \n",
    "               color='mediumpurple', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Component ID')\n",
    "axes[1, 0].set_ylabel('Size')\n",
    "axes[1, 0].set_title('Size of Top 20 Connected Components')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# community sizes (lpa)\n",
    "axes[1, 1].bar(range(len(comm_sizes_viz)), \n",
    "               comm_sizes_viz['count'], \n",
    "               color='orange', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Community ID')\n",
    "axes[1, 1].set_ylabel('Size')\n",
    "axes[1, 1].set_title('Size of Top 20 Communities')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "# pagerank vs in-degree\n",
    "axes[1, 2].scatter(pr_indeg_viz['inDegree'], pr_indeg_viz['pagerank'], \n",
    "                   alpha=0.5, s=20, color='crimson')\n",
    "axes[1, 2].set_xlabel('In-Degree')\n",
    "axes[1, 2].set_ylabel('PageRank')\n",
    "axes[1, 2].set_title('PageRank vs Citations Correlation')\n",
    "axes[1, 2].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('arxiv_graph_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualizations saved to 'arxiv_graph_analysis.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5426534",
   "metadata": {},
   "source": [
    "The visualizations further confirm the network's highly fragmented structure:\n",
    "1. **Sparsity:** the 'In-Degree' and 'Out-Degree' histograms show a big decline. The majority of papers only have 1 or 2 connections and the maximum is 9.\n",
    "2. **Top influencers:** the paper `0804.0079` stands out as the clear leader. The balanced scores of the other top articles suggest they are probably the leaders of their own separate communities.\n",
    "3. **Small communities:** The 'Component' and 'Community' size charts reveal that the largest groups contain less than 15 papers. This visually cinfirms that the network consist of small isolated communities, rather than a big cohesive web.\n",
    "4. **Influence**: the scatter plot shoes a positive correlation between the number of citations and the PageRank score. However, the variance indicates that PageRank is capurating a structural importance that goes further than just popularity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd007b",
   "metadata": {},
   "source": [
    "### Analysis by category\n",
    "\n",
    "In this final step, we aggregate the results by field in order to understand which areas carry the most weight. We extract the primary category from each paper and calculate the average PageRank score for that group. This allows us to see which specific areas tend to produce the most influential articles in this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 10: ANALYSIS BY SCIENTIFIC CATEGORIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# load vertices and pagerank from HDFS\n",
    "vertices_cat = spark.read.parquet(\"hdfs:///output/citation_graph/nodes_filtered\")\n",
    "pagerank_cat = spark.read.parquet(\"hdfs:///output/citation_graph/pagerank_results\")\n",
    "\n",
    "# extract primary category (distributed operation)\n",
    "from pyspark.sql.functions import split\n",
    "vertices_with_primary = vertices_cat.withColumn(\n",
    "    'primary_category',\n",
    "    split(col('categories'), ' ').getItem(0)\n",
    ")\n",
    "\n",
    "# merge with pagerank (distributed join)\n",
    "pagerank_with_cat = pagerank_cat.join(\n",
    "    vertices_with_primary.select('id', 'primary_category'),\n",
    "    'id'\n",
    ")\n",
    "\n",
    "# calculate average pagerank by category (distributed aggregation)\n",
    "category_pagerank = pagerank_with_cat.groupBy('primary_category').agg(\n",
    "    F.mean('pagerank').alias('mean'),\n",
    "    F.count('pagerank').alias('count')\n",
    ").orderBy(col('mean').desc())\n",
    "\n",
    "# get top 20 categories\n",
    "top_categories = category_pagerank.limit(20).toPandas()\n",
    "\n",
    "print(\"\\nTop 20 categories by average PageRank:\")\n",
    "print(top_categories.to_string(index=False))\n",
    "\n",
    "# visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_categories)), top_categories['mean'], color='teal')\n",
    "plt.yticks(range(len(top_categories)), top_categories['primary_category'])\n",
    "plt.xlabel('Average PageRank')\n",
    "plt.ylabel('Category')\n",
    "plt.title('Average Importance by Scientific Category')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nGraph saved to 'category_importance.png'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8363f1e3",
   "metadata": {},
   "source": [
    "The bar chart shows that Computer Science and Statistics fileds have the highest average PageRank score.\n",
    "\n",
    "However, the text output reveals a very important detail: the top categories have very few papers, meaning their high score relies on just a couple of articles, while Statistical Methodology maintains a very high average score (3.14) across 87 articles. This confirms once again that Statistics is the central and most interconnected topic in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c61b2",
   "metadata": {},
   "source": [
    "### Final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfa6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# load summary statistics from HDFS\n",
    "num_nodes_filtered = spark.read.parquet(\"hdfs:///output/citation_graph/nodes_filtered\").count()\n",
    "num_edges_filtered = spark.read.parquet(\"hdfs:///output/citation_graph/edges_filtered\").count()\n",
    "\n",
    "# component statistics\n",
    "comp_stats = spark.read.parquet(\"hdfs:///output/citation_graph/connected_components\") \\\n",
    "    .groupBy(\"component\").count().orderBy(col(\"count\").desc()).limit(1).collect()\n",
    "total_components = spark.read.parquet(\"hdfs:///output/citation_graph/connected_components\") \\\n",
    "    .select(\"component\").distinct().count()\n",
    "\n",
    "# community statistics\n",
    "comm_stats = spark.read.parquet(\"hdfs:///output/citation_graph/label_propagation\") \\\n",
    "    .groupBy(\"label\").count().orderBy(col(\"count\").desc()).limit(1).collect()\n",
    "total_communities = spark.read.parquet(\"hdfs:///output/citation_graph/label_propagation\") \\\n",
    "    .select(\"label\").distinct().count()\n",
    "\n",
    "# top cited paper\n",
    "top_cited_stats = spark.read.parquet(\"hdfs:///output/citation_graph/in_degrees\") \\\n",
    "    .orderBy(col(\"inDegree\").desc()).limit(1).collect()\n",
    "\n",
    "# top pagerank\n",
    "top_pr_stats = spark.read.parquet(\"hdfs:///output/citation_graph/pagerank_results\") \\\n",
    "    .orderBy(col(\"pagerank\").desc()).limit(1).collect()\n",
    "\n",
    "print(f\"\"\"\n",
    "GRAPH STATISTICS:\n",
    "  - Papers analyzed: {num_nodes_filtered:,}\n",
    "  - Total citations: {num_edges_filtered:,}\n",
    "  - Average density: {num_edges_filtered/num_nodes_filtered:.2f} citations/paper\n",
    "  \n",
    "COMPONENTS AND COMMUNITIES:\n",
    "  - Connected components: {total_components}\n",
    "  - Size of main component: {comp_stats[0]['count']:,} papers\n",
    "  - Detected communities (LPA): {total_communities}\n",
    "  \n",
    "MOST IMPORTANT PAPERS:\n",
    "  - Most cited paper: {top_cited_stats[0]['inDegree']} citations\n",
    "  - Max PageRank: {top_pr_stats[0]['pagerank']:.4f}\n",
    "  \n",
    "GENERATED FILES IN HDFS:\n",
    "  - hdfs:///output/citation_graph/nodes_filtered - Filtered nodes\n",
    "  - hdfs:///output/citation_graph/edges_filtered - Filtered edges\n",
    "  - hdfs:///output/citation_graph/pagerank_results - PageRank results\n",
    "  - hdfs:///output/citation_graph/connected_components - Components\n",
    "  - hdfs:///output/citation_graph/label_propagation - Communities\n",
    "  - hdfs:///output/citation_graph/in_degrees - Citation counts\n",
    "  - hdfs:///output/citation_graph/out_degrees - Reference counts\n",
    "  \n",
    "LOCAL VISUALIZATIONS:\n",
    "  - arxiv_graph_analysis.png - Main visualizations\n",
    "  - category_importance.png - Category analysis\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf5171e",
   "metadata": {},
   "source": [
    "### Close Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0293149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close spark session\n",
    "print(\"\\nClosing Spark session...\")\n",
    "spark.stop()\n",
    "print(\"Session closed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
